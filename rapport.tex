\documentclass{report}
\usepackage[utf8]{inputenc}
\RequirePackage[numbers]{natbib}
\input{defs.sty}
\input{init.sty}
\title{SIGMA205 : Prédiction d'une série temporelle localement
stationnaire}
\author{Thomas EBOLI et Amaury DURAND}
\begin{document}
\maketitle
\tableofcontents
\pagebreak
%\chapter*{Notations}
%\pagebreak
\chapter{Rappels sur les processus AR}
\paragraph{Equation AR(p) :} 
On s'intéresse à l'équation AR(p) définie pour $p\geq 1$ et $(X_t)_{t\in \zset}$ un processus aléatoire à valeurs dans $\cset$ par 
\begin{equation}\label{eq:AR}
X_t = \sum_{i=1}^p \theta_i X_{t-i} + \epsilon_t
\tag{AR}
\end{equation}
où $\theta_1,\cdots,\theta_p \in \cset$ et $\epsilon \sim BB(0,\sigma^2)$ (i.e $\epsilon$ est un processus stationnaire au second ordre, centré de fonction d'autocovariance $\gamma$ vérifiant $\gamma(0)=\sigma^2 > 0$ et $\forall s\neq 0, \gamma(s)=0$)
\section{Construction d'une solution stationnaire au second ordre}
\begin{Thm}
Soit $\Theta(z)=1-\sum_{k=1}^p \theta_k z^k$, supposons que $\forall |z|=1, \Theta(z)\neq 0$ \\
Alors il existe une solution stationnaire au second ordre de \eqref{eq:AR} 
\end{Thm}
\begin{proof}
Pour prouver cela on a besoin du lemme suivant :
\begin{Lem}
Pour $\alpha \in \ell^1$ et $X$ un processus tel que $\sup_t \EE[|X_t|] < +\infty$ on appelle filtrage de $X$ le processus : 
\[ F_\alpha(X) = \left( \sum_{k\in \zset} \alpha_k X_{t-k} \right)_{t\in \zset} \]
Alors si $\alpha,\beta \in \ell^1$ on a $F_\alpha (F_\beta (X)) = X$ si $\alpha \star \beta = \delta$ \\
De plus $\alpha \star \beta = \delta \iff \forall |z|=1 \, \left( \sum_{z\in \zset} \alpha_k z^k \right) \left( \sum_{z\in \zset} \beta_k z^k \right) =1 $ \\
Enfin si $X$ est stationnaire au second ordre $F(X)$ l'est aussi.
\end{Lem}
\begin{Rque}
$F_\alpha = \sum_{k\in \zset} \alpha_i B^k$ où $B$ est l'opérateur de shift $ B : (x_t)_{t\in \zset} \mapsto (x_{t-1})_{t\in \zset}$
\end{Rque}

Le polynôme $\Theta$ peut s'écrire : $\Theta(z)=\prod_{k=1}^p (1-u_k z)$ où les $u_k$ sont les inverses des racines de $\Theta$. L'équation \eqref{eq:AR} s'écrit $\Theta(B)(X)=\epsilon$. Or avec la factorisation obtenue on a : 
\[
\Theta(B) = (1-u_1 B) \circ \cdots \circ (1-u_p B) = F_{\alpha^{(1)}} \circ \cdots \circ F_{\alpha^{(p)}}
\]
où $\alpha_k^{(l)} = \piecewise{
1 & k=0 \\
-u_l & k=1 \\
0 & \text{sinon} \\}
$ \\
Le but désormais est de chercher pour tout $l \in \llbracket 1, p \rrbracket$ un $\beta^{(l)}$ tel que $\alpha^{(l)} \star \beta^{(l)} = \delta$ pour pouvoir inverser la relation. On sait d'après la remarque qu'il suffit de trouver $\beta^{(l)}$ tel que $\frac{1}{1-u_l z} = \sum_{k \in \zset} \beta_k z^k$ pour tout $|z|=1$ \\
On sait de plus que $\forall l \in \llbracket 1, p \rrbracket$ $|u_l| \neq 1$ par hypothèse sur les racines de $\Theta$.\\
Prenons $l\in \llbracket 1,p \rrbracket$ alors deux cas sont possibles :
\begin{itemize}
\item si $|u_l|< 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} = \sum_{k \geq 0} u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
u_l^k & k \geq 0 \\
0 & \text{sinon} \\
}$
\item si $|u_l|> 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} \frac{-u_l^{-1} z^{-1}}{1-u_l^{-1} z^{-1}} = \sum_{k \leq -1} -u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
-u_l^k & k \leq -1 \\
0 & \text{sinon} \\
}$
\end{itemize}
Finalement prenons $F = F_{\beta^{(p)}} \circ \cdots \circ F_{\beta^{(1)}}$, on prends alors 
\[
X = F(\epsilon) 
\]
\end{proof}

\section{Prédiction}
\begin{Prop}
On se donne $X$ stationnaire au second ordre vérifiant \eqref{eq:AR}. On suppose de plus que $X$ est causal i.e $\Theta(z) \neq 0$ pour tout $|z| \leq 1$
On note $\mathcal{H}_t^X = \overline{\mathrm{Vect}\ens{X_s, s\leq t}}$. Alors
\[ \hat{X}_{t+1} = proj(X_{t+1} | \mathcal{H}_t^X ) = \sum_{k=1}^p \theta_k X_{t+1-k} \]
\end{Prop}
\begin{proof}
$X_{t+1} = \sum_{k=1}^p \theta_k X_{t+1-k} +\epsilon_{t+1}$ de plus $\forall k \in \llbracket 1,p \rrbracket \, X_{t+1-k} \in \mathcal{H}_t^X$ donc
\[ \hat{X}_{t+1} = \sum_{k=1}^p \theta_k X_{t+1-k} + proj(\epsilon_{t+1} | \mathcal{H}_t^X ) \]
Or $X$ est causal donc dans la construction de $X$ (cf preuve d'avant) on a $\Theta(z)= \prod_{l=1}^p (1-u_l z)$ où $\forall l |u_l|<1$ ainsi les $\beta^{(l)}$ correspondants sont tous à support dans $\nset$ ce qui entraine que $\psi = \beta^{(p)} \star \cdots \star \beta^{(1)}$ est aussi à support dans $\nset$ et donc 
\[ X_t = \sum_{k=0}^{+\infty} \psi_k \epsilon_{t-k}\] On en déduit que $\mathcal{H}_t^X = \mathcal{H}_t^\epsilon$ et donc comme $\epsilon$ est un bruit blanc $proj(\epsilon_{t+1} | \mathcal{H}_t^X ) = proj(\epsilon_{t+1} | \mathcal{H}_t^\epsilon ) = 0$ d'où la solution
\end{proof}

\section{Algorithme de Levinson-Durbin}
On rappelle ici l'algorithme de Levinson Durbin et quelques conséquences. Le cadre d'étude est le suivant : \\
On se donne un processus stationnaire au second ordre, centré $X$ de fonction d'autocovariance $\gamma$. On suppose que pour tout $p\geq 1$ la matrice de covariance de $\mb{X}_{t,p} = [X_t, \cdots , X_{t-p+1}]^T$ $\Gamma_p = \EE[\mb{X}_{t,p} \mb{X}_{t,p}^T]$ est \textbf{inversible}. On rappelle que cette matrice est la matrice de Toeplitz suivante :
$$
\Gamma_p = \begin{bmatrix}
\gamma(0) & \gamma(-1) & \cdots & \gamma(2-p) &\gamma(1-p) \\
\gamma(1) & \gamma(0) & \ddots  &  &\gamma(2-p) \\
\vdots & \ddots & \ddots & \ddots &\vdots \\
\gamma(p-2) &  & \ddots & \ddots & \gamma(-1) \\
\gamma(p-1)& \gamma(p-2)&  \cdots &  \gamma(1)& \gamma(0) \\
\end{bmatrix}
$$
Remarquons le lemme suivant :
\begin{Lem}
Il existe $t \in \zset$ tel que $X_t \in \Vect(X_{t-1},\cdots,X_{t-p+1})$ si et seulement si $\Gamma_p$ est non inversible
\end{Lem}
\begin{proof}
En effet $\Gamma_p$ est la matrice de Gram de $(X_t,\cdots,X_{t-p+1})$ associée au produit scalaire $\inner{X,Y} = \EE[X\overline{Y}]$ qui est inversible si et seulement si $(X_t,\cdots,X_{t-p+1})$ est libre. 
\end{proof}
On note pour tout $p\geq 1$, pour tout $k\in \iseg{1,p}$, $\theta_{k,p}$ les coefficients de prédiction direct d'ordre $p$ de $X_t$ à partir de son passé et $\sigma_p^2$ l'erreur de prédiction i.e 
$$
\proj(X_t |\calH_{t-1,p}^X)= \sum_{k=1}^p \theta_{k,p} X_{t-k} 
\mbox{ et } 
\sigma_p^2 = \norm{X_t - \proj(X_t | \calH_{t-1,p}^X)}_2^2
$$
où $\calH_{t-1,p}^X = \vect(X_{t-1}, \cdots, X_{t-p})$ \\
L'algorithme de Levinson permet de trouver tous les $\left( \theta_{k,p} \right)_{p \in \iseg{1,d}, k\in \iseg{1,p}}$ où $d$ est un ordre auquel on s'arrête, à partir des $\gamma(1),\cdots, \gamma(d)$. Il est basé sur un calcul de $(\theta_{1,p+1}, \cdots, \theta_{p,p})$ connaissant $(\theta_{1,p-1}, \cdots, \theta_{p-1,p-1})$\\
\begin{algorithm}[H]
 \KwData{$\gamma(1),\cdots,\gamma(d)$}
 \KwResult{$(\theta_{i,p})_{p\in \iseg{1,d},i\in \iseg{1,p}}$ }
 \KwInit{ $\kappa_1 = \frac{\gamma(1)}{\gamma(0)}$, $\theta_{1,1} = \kappa_1$, $\sigma_1^2 = \gamma(0)(1-\kappa_1^2)$\;}
 \For{$p = 2,\cdots,d$}{
 	$\kappa_p = \sigma_{p-1}^{-2} \left( \gamma(p) - \sum_{k=1}^{p-1} \theta_{k,p-1} \gamma(p-k) \right)$ \;	
   $\theta_{p,p} = \kappa_p$ \;
   $\sigma_p^2 = \sigma_{p-1}^2 (1-\kappa_p^2)$ \;
 	\For{$m = 1\cdots p-1$}{
 	$\theta_{m,p} = \theta_{m,p-1} - \kappa_p \theta_{p-m,p-1}$
 	}
 }
 \caption{Levinson-Durbin}
 \label{algo:levinson}
\end{algorithm}
\begin{Prop}
Les coefficient $\kappa_p$ s'appellent coefficients d'autocorrélation partielle et vérifient :
$$
\forall p\geq 1, \abs{\kappa_p} \leq 1
$$
\end{Prop}
\begin{Prop}\label{prop:Gamma_inv}
Soit $p\geq 1$. Si pour tout $k \geq p, \kappa_k \in \osego{-1,1}$ alors $\forall k\leq p, \Gamma_p$ est inversible.
\end{Prop}
On peut alors en générant une famille $(\kappa_1, \cdots , \kappa_d) \in \osego{-1,1}^d$ calculer des coefficients $\theta$ en inversant les équations de Levinson-Durbin grâce à l'algorithme suivant \\
\begin{algorithm}[H]
 \KwData{$(\kappa_1, \cdots , \kappa_d) \in \osego{-1,1}^d$}
 \KwResult{$(\theta_{i,p})_{p\in \iseg{1,d},i\in \iseg{1,p}}$ }
 \KwInit{ 
 	\For{$p = 1, \cdots, d$}{
 	$\theta_{p,p} = \kappa_p$\;
 	}
 }
\For{$p = 2, \cdots , d$}{
	\For{$m = 1, \cdots , p-1$}{
		$\theta_{m,p} = \theta_{m,p-1} - \kappa_p \theta_{p-m,p-1}$\;
	}
}
 \caption{Construction des $\theta$ à partir  des $\kappa$}
 \label{algo:construction}
\end{algorithm}
\begin{Rque}
Dans cet algorithme, à chaque étape on calcule les $(\theta_{1,p}, \cdots, \theta_{p-1,p})$ grâce aux $(\theta_{1,p-1}, \cdots, \theta_{p-1,p-1})$ calculés à l'étape d'avant :
\begin{itemize}
\item $p=2$ : $\theta_{1,2} = \theta_{1,1} - \kappa_2 \theta_{1,1}$
\item $p=3$ : $\theta_{1,3} = \theta_{1,2} - \kappa_3 \theta_{2,2}$ et $\theta_{2,3} = \theta_{2,2} - \kappa_3 \theta_{1,2}$\\
...
\end{itemize} 
\end{Rque}
\begin{Thm}
\label{thm:kappa}
Soit $d \geq 1$. Alors quels que soient $(\kappa_1,\cdots, \kappa_d)\in \osego{-1,1}^d$, $\forall p \in \iseg{1,d}$, les coefficients $(\theta_{1,p}, \cdots, \theta_{p,p})$ construits par l'algorithme \ref{algo:construction} sont les coefficients d'un processus $AR(p)$ causal i.e 
$$
1-\sum_{k=1}^p \theta_{k,p} z^k \neq 0 \,\, \forall \abs{z} \leq 1
$$
\end{Thm}
\begin{proof}~\\
\textbf{Voyons tout d'abord le cas où $p=1$ :} \\
On a $\theta_{1,1} = \kappa_1 = \frac{\gamma(1)}{\gamma(0)} \in \osego{-1,1} \setminus \ens{0}$. Donc l'unique racine de $1-\theta_{1,1}z$ est $\frac{1}{\theta_{1,1}}$ de module strictement supérieur à $1$ \\
\textbf{Cas p quelconque :} Soit maintenant $p \in \iseg{1,d}$ quelconque alors écrivons 
$$
1- \sum_{k=1}^p \theta_{k,p} z^k = \prod_{k=1}^p (1-\nu_{k,p} z)
$$
où les $\nu_{k,p}$ sont les inverses des racines du polynôme. Le résultat cherché équivaut à $\forall k\in {1,\cdots p}, \abs{\nu_{k,p}} < 1$. \\
Construisons une bijection liant les $(\nu_{k,p})_{k\in \iseg{1,p}}$ aux $(\theta_{k,p})_{k\in \iseg{1,p}}$ : \\
La première idée serait de considérer $\bftheta_p=[\theta_{1,p},\cdots,\theta_{p,p}]^T \mapsto [\nu_{1,p},\cdots,\nu_{p,p}]^T$ mais cela ne définit pas une application car une permutation des éléments dans le vecteur de droite donne le même polynôme et donc on aurait deux images pour le même $\bftheta_p$. \\
La deuxième idée serait de considérer alors $\bftheta \mapsto \ens{ \nu_{k,p}, k\in \iseg{1,p}}$ mais le problème dans cette définition est que si il existe $i \neq j$ tels que $\nu_{i,p}=\nu_{j,p}$, cela a une importance dans le polynôme (racine multiple) mais cette redondance n'est pas visible dans $\ens{ \nu_{k,p}, k\in \iseg{1,p}}$. \\
Finalement une vision qui fonctionne est de considérer $\Phi : \bftheta_p = (\theta_{1,p}, \cdots, \theta_{p,p}) \mapsto \overline{\nu}_p = \sum_{k=1}^p \delta_{\nu_{k,p}}$. Le fait de considérer la mesure $\sum_{k=1}^p \delta_{\nu_{k,p}}$ tient compte du fait qu'une interversion des $\nu_{k,p}$ ne change pas la valeur du polynôme et n'annule pas les redondances. \\
Soit $X$ le processus stationnaire au second ordre associé aux $\kappa_1,\cdots,\kappa_p$.
Montrons que $\displaystyle \overline{\nu}_p \mapsto \EE[\abs{\left( \prod_{k=1}^p (1-\nu_{k,p} B) X\right)_t}^2]$ admet un unique minimiseur : \\
Il suffit de considérer $\bftheta_p = \Phi^{-1} (\overline{\nu}_p)$ et alors 
$$
\EE[\abs{\left( \prod_{k=1}^p (1-\nu_{k,p} B) X\right)_t}^2]
=
\EE[\abs{X_t-\sum_{k=1}^p \theta_{k,p} X_{t-k}}^2]
$$
Ce problème a bien une unique solution $\bftheta_p^\ast$ car $\Gamma_p$ est inversible (puisque pour tout $k \in \iseg{1,p}, \kappa_k \in \osego{-1,1}$). Ainsi on a bien par bijection une unique solution 
$$
\overline{\nu}_p^\ast = \Phi(\bftheta_p^\ast) = \argmin_{\overline{\nu_p}} \EE[\abs{\left( \prod_{k=1}^p (1-\nu_{k,p} B) X\right)_t}^2]
$$
Ecrivons maintenant
\begin{equation}\label{eq:nu_p}
\overline{\nu}_p^\ast = \sum_{k=2}^p \delta_{\nu_{k,p}^\ast} + \delta_{\nu_{1,p}^\ast}
\end{equation}
Alors 
$$
\nu_{1,p}^\ast = \argmin_{\nu \in \cset} \EE[\abs{\left( (1-\nu B) \circ \prod_{k=2}^p (1-\nu_{k,p}^\ast B) X\right)_t}^2]
$$
En effet si cela n'était pas le cas i.e si $\tilde{\nu} = \argmin_{\nu \in \cset} \EE[\abs{\left( (1-\nu B) \circ \prod_{k=2}^p (1-\nu_{k,p}^\ast B) X\right)_t}^2]$ était différent de $\nu_{1,p}$ on aurait 
$$
\EE[\abs{\left( (1-\tilde{\nu} B) \circ \prod_{k=2}^p (1-\nu_{k,p}^\ast B) X\right)_t}^2]
< 
\EE[\abs{\left( (1-\nu_{k,p}^\ast B) \circ \prod_{k=2}^p (1-\nu_{k,p}^\ast B) X\right)_t}^2]
$$
donc $\sum_{k=2}^p \delta_{\nu_{k,p}^\ast} + \delta_{\tilde{\nu}}$ fournirait une valeur plus petite que celle fournie par $\overline{\nu}_p^\ast$. Ceci est impossible par définition de ce dernier. \\
Considérons maintenant $Y = \prod_{k=2}^p (1- \nu_{k,p}^\ast B) X$. Par filtrage d'un processus stationnaire au second ordre centré, $Y$ est stationnaire au second ordre centré. De plus on a
\begin{eqnarray*}
\nu_{1,p}^\ast 
&=& \argmin_{\nu \in \cset} \EE[\abs{\left((1-\nu B)Y\right)_t}^2] \\
&=& \argmin_{\nu \in \cset} \EE[\abs{Y_t - \nu Y_{t-1}}^2]
\end{eqnarray*} 
On est ramené à un cas d'ordre 1 dont la solution est le coefficient de prédiction d'ordre 1 associé à $Y$ qui vaut (cf algorithme de Levinson \ref{algo:levinson}) $\frac{\gamma_Y(1)}{\gamma_Y(0)}$. Donc 
$$
\abs{\nu_{1,p}^\ast} = \abs{\frac{\gamma_Y(1)}{\gamma_Y(0)}} \leq 1
$$ 
Il reste à montrer que cette inégalité est stricte. \\
$\abs{\nu_{1,p}^\ast} = 1 \iff \gamma_Y(1)= \gamma_Y(0) \iff \EE[Y_1 \overline{Y}_0] = \EE[\abs{Y_0}^2] \iff Y_1 \in \Vect(Y_0)$ (cas d'égalité dans Cauchy-Schwarz). \\
On a $Y_0 \in \Vect(X_0,\cdots X_{1-p})$ donc 
$$
\Vect(Y_0) \subset \Vect(X_0, \cdots X_{1-p})
$$
De plus en développent le produit dans l'expression de $Y$, on peut écrire $Y_1 = X_1 + Z$ où $Z \in \Vect(X_0,\cdots,X_{2-p}) \subset \Vect(X_0, \cdots X_{1-p})$. Ainsi 
$$
Y_1 \in \Vect(Y_0)  \Rightarrow X_1 = Y_1 - Z \in \Vect(X_0, \cdots X_{1-p}) \Rightarrow \Gamma_p \mbox{ non inversible }
$$
Ce qui est contradictoire. \\
On a donc montré que $\abs{\nu_{1,p}^\ast} < 1$ mais de plus dans l'équation \eqref{eq:nu_p}, l'ordre des coefficients $\nu_{k,p}^\ast$ n'a pas d'importance, ainsi avoir montré l'inégalité pour $\nu_{1,p}^\ast$ la prouve aussi pour les autres (il suffit d'alterner $\nu_{1,p}^\ast$ avec un autre coefficient). \\
Finalement on a bien $\forall k\in \iseg{1,p}, \abs{\nu_{1,p}^\ast} < 1$

\end{proof}
\section{Densité spectrale de puissance}
On rappelle la formule de la DSP dans le cas d'un modèle AR(p) :
\begin{Def}[DSP]
La DSP est définie pour $\lambda \in [-\pi, \pi[$ 
$$
S_x(\lambda) = \frac{\sigma^2}{2\pi\abs{\Theta\left( e^{-i\lambda} \right)}^2}
$$
où $\Theta(z) = 1 - \sum_{k=1}^p \theta_k z^k$ \\
\end{Def}
\begin{Prop}\label{prop:racines_phase}
Considérons $z_1,\cdots, z_p$ les inverses des racines de $\Theta(z)$ alors en notant $\forall n\in \iseg{1,p}, z_n = \rho_n e^{i \phi_n}$ avec $0 < \rho_n <1$ et $\phi_n \in [-\pi, \pi[$ on a, si $\forall n\in \iseg{1,p}, \rho_n$ est assez proche de $1$
$$
S_x(\lambda) \mbox{ admet un pic en $\lambda$ } \iff \exists n\in \iseg{1,p} \, \lambda = \phi_n
$$
\end{Prop}
\begin{proof}
$$
\abs{\Theta\left( e^{-i\lambda} \right)}^2 = \prod_{n=1}^p \abs{1 - \rho_n e^{i(\phi_n-\lambda)}}^2
$$
Or $\forall n \in \iseg{1,p}$
$$
\abs{1 - \rho_n e^{i(\phi_n-\lambda)}}^2 = 1 - 2\rho_n \cos(\phi_n-\lambda) + \rho_n^2
$$
est minimal pour $\lambda = \phi_n \mod 2\pi$ \\
Ceci ne veut pas  dire que le produit est minimal ssi $\exists n\in \iseg{1,p}, \lambda = \phi_n(u) \mod 2\pi$. Mais si $\forall n\in \iseg{1,p}, \rho_n$ est assez proche de $1$, pour chaque racine, les contributions des autres racines sont négligeables.
\end{proof}

\paragraph{Cas particulier où p=1 :}
Dans le cas $p=1$ quelle que soit la valeur de $\rho$, $\abs{1-\rho e^{i(\phi-\lambda)}}^2$ est minimal ssi $\lambda = \rho$

\paragraph{Cas particulier où p=2 à racines complexes conjuguées :}
On écrit les deux inverses des racines : $z_1=\overline{z}_2 = \rho e^{i\phi}$, avec $0<\rho<1, \phi \in \sego{-\pi,\pi}$ alors en développant le produit, $\forall \lambda \in \sego{-\pi,\pi}$
$$
f(\lambda) \deq \abs{\Theta\left( e^{-i\lambda}\right)}^2 = 1 + \rho^4 + 4\rho^2 \cos^2{\phi} - 4\rho(1+\rho^2)\cos{\phi} \cos{\lambda} + 2\rho^2\cos{2\lambda}
$$
On dérive :
$$
f'(\lambda) = 4\rho \sin(\lambda) \left[ (1+\rho^2) \cos{\phi} - 2\rho \cos{\lambda} \right]
$$
ainsi 
$$
f'(\lambda)=0 \iff \lambda = 0 \mbox{ ou } \cos{\lambda} = \frac{1+\rho^2}{2\rho} \cos{\phi}
$$
La seconde condition n'est possible que si $ -1\leq \frac{1+\rho^2}{2\rho} \cos{\phi} \leq 1$ ce qui se traduit par 
\begin{equation}\label{eq:phaseAR2}
\piecewise{
\phi \in \seg{\arccos\left( \frac{2\rho}{1+\rho2} \right), \arccos\left( \frac{-2\rho}{1+\rho2} \right)} & \mbox{ (cas où $\phi \in \sego{0,\pi}$)} \\
\mbox{ ou } & \\
\phi \in \seg{-\arccos\left( \frac{-2\rho}{1+\rho2} \right), -\arccos\left( \frac{2\rho}{1+\rho2} \right)} & \mbox{ (cas où $\phi \in \seg{-\pi,0}$)} \\
}
\end{equation}
On a alors deux cas possibles :
\begin{enumerate}
\item Si $\phi$ vérifie \eqref{eq:phaseAR2} : alors 
$$
f'(\lambda)=0 \iff \lambda = 0 \mbox{ ou } \lambda = \pm \arccos\left(\frac{1+\rho^2}{2\rho} \cos{\phi}\right)
$$
et on a le tableau de variations suivant : \\
\begin{center}
\begin{tabular}{c|cccccccccc}
$\lambda$ & $-\pi$ & & $-\arccos\left(\frac{1+\rho^2}{2\rho} \cos{\phi}\right)$ & & $0$ & & $\arccos\left(\frac{1+\rho^2}{2\rho} \cos{\phi}\right)$ & & $\pi$  \\
\hline
$f'(\lambda)$ & & $-$ & $0$ & $+$ & $0$ & $-$ & $0$ & $+$ & \\
\hline
$f$ & & $\searrow$ & & $\nearrow$ & & $\searrow$ & & $\nearrow$ \\
\end{tabular}
\end{center}
\item Si $\phi$ ne vérifie pas \eqref{eq:phaseAR2} alors là encore deux cas possibles
\begin{enumerate}
\item Si $\frac{1+\rho^2}{2\rho} \cos{\phi} > 1$ i.e $\phi < \arccos\left( \frac{2\rho}{1+\rho^2} \right)$ ou $\phi > -\arccos\left( \frac{2\rho}{1+\rho^2} \right)$ alors $(1+\rho^2)\cos{\phi} - 2\rho \cos{\lambda} >0$ et on a le tableau suivant :
\begin{center}
\begin{tabular}{c|ccccc}
$\lambda$ & $-\pi$ & & $0$ & & $\pi$  \\
\hline
$f'(\lambda)$ & & $-$ & $0$ & $+$ & \\
\hline
$f$ & & $\searrow$ & & $\nearrow$ & \\
\end{tabular}
\end{center}
\item Si $\frac{1+\rho^2}{2\rho} \cos{\phi} < -1$ i.e $\phi < -\arccos\left( \frac{-2\rho}{1+\rho^2} \right)$ ou $\phi > \arccos\left( \frac{-2\rho}{1+\rho^2} \right)$ alors $(1+\rho^2)\cos{\phi} - 2\rho \cos{\lambda} <0$ et on a le tableau suivant :
\begin{center}
\begin{tabular}{c|ccccc}
$\lambda$ & $-\pi$ & & $0$ & & $\pi$  \\
\hline
$f'(\lambda)$ & & $+$ & $0$ & $-$ & \\
\hline
$f$ & & $\nearrow$ & & $\searrow$ & \\
\end{tabular}
\end{center}
\end{enumerate}
\end{enumerate}
Voyons ce qu'il se passe quand $\rho \to 1$ : \\
$\frac{1+\rho^2}{2\rho} \cos{\phi}  \xrightarrow[\rho \to 1]{} \cos{\phi}$ donc on se retrouve dans le cas 1 et $\arccos\left(\frac{1+\rho^2}{2\rho} \cos{\phi}\right) \xrightarrow[\rho \to 1]{} \phi$. On aura donc bien nos pics à $\lambda = \phi$ et $\lambda = -\phi$
\chapter{Processus TVAR}
\section{Construction d'une solution stable}
\paragraph{Equation TVAR :}
\begin{equation} \label{eq:TVAR}
X_t = \sum_{i=1}^p \theta_i(t) X_{t-i} + \sigma(t) \epsilon_t
\tag{TVAR}
\end{equation}
où $\epsilon \overset{\rm{iid}}{\sim} BB(0,1)$.
\paragraph{Condition de stabilité :}
Le critère qui nous intéresse est d'avoir une solution de \eqref{eq:TVAR} vérifiant la condition de stabilité suivante :
\begin{equation} \label{eq:stabilite}
\sup_{t\in \zset} \EE[|X_t|^2] < +\infty
\tag{S}
\end{equation}
\subsection{Cas simple}
On considère ici $\sigma(t)=1 \, \forall t\in \rset$ et $\theta_i(t) = \piecewise{
0 & t<0 \\
\theta_i & t \geq 0 \\ 
} $. 
\begin{Prop}\label{prop:cas_simple_unicite}
Dans ce cas particulier, l'équation \eqref{eq:TVAR} admet une unique solution
\end{Prop} 
\begin{proof}
Tout d'abord pour $t < 0$ on a $X_t = \epsilon_t$ \\
Pour $t \geq 0$, notons $\mb{X}_k = [ X_k, \cdots , X_{k-p+1} ]^T$, $\mb{e}_1 = [1,0, \cdots, 0]^T$ et $A = \begin{pmatrix}
\theta_1 & \theta_2 & \cdots & \cdots & \theta_p \\
1 & 0 & \cdots & \cdots & 0 \\
0 & \ddots & \ddots & & \vdots \\
\vdots & \ddots & \ddots & \ddots & 0 \\
0 & \cdots & 0 & 1 & 0
\end{pmatrix}$. Alors l'équation \eqref{eq:TVAR} s'écrit : 
\[ \mb{X}_t = A\mb{X}_{t-1} + \epsilon_t \mb{e}_1 \]
En itérant pour $t-1, t-2$ etc, on obtient 
\[ \forall k\geq 0 \,  \mb{X}_t = A^{k+1} \mb{X}_{t-k-1} + \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  \]
Considérons $k \geq t$ alors $\mb{X}_{t-k-1} = [ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]^T$. Ainsi 
\begin{equation}\label{eq:X}
\forall k\geq t \,  \mb{X}_t = A^{k+1} \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix} 
+ \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  
\end{equation}
ce qui fournit une définition $X_t = \mb{e}_1^T \mb{X}_t$ unique
\end{proof}

On cherche alors une condition sur les $(\theta_i)_{i=1}^p$ pour cette solution vérifie la condition de stabilité \eqref{eq:stabilite}


\subsubsection{Cas où p=1}
\begin{Prop}
Si $p=1$ on note $\theta(t)= \piecewise{
0 & t<0 \\
\theta & t \geq 0 \\} $ et \eqref{eq:TVAR} devient $X_t = \theta(t)X_{t-1} + \sigma(t) \epsilon_t$. La solution de l'équation vérifie la condition de stabilité \eqref{eq:stabilite} si et seulement si $|\theta| < 1$
\end{Prop}
\begin{proof}
Si $t < 0\, X_t = \epsilon_t$ donc $\sup_{t < 0} \EE[|X_t|^2] = 1$. Si $t\geq 0$, la formule de $X_t$ donnée par \eqref{eq:X} se traduit pour $p=1$ par
$ \forall k \geq t \,  X_t = \theta^{k+1}\epsilon_{t-k-1} + \sum_{j=0}^k \theta^j \epsilon_{t-j} $ i.e $\forall k \geq t \, X_t = \sum_{j=0}^{k+1} \theta^j\epsilon_{t-j}$.
Ainsi 
\[ \forall t \in \nset, \, \forall k \geq t \, \EE[|X_t|^2] = \sum_{i=0}^{k+1} \sum_{j=0}^{k+1} \theta^i \overline{\theta}^j \EE[\epsilon_{t-i} \epsilon_{t-j}]= \sum_{i=0}^{k+1} |\theta|^{2i} \]
Ainsi en faisant tendre $k$ vers $+\infty$ on obtient 
 \[ \forall t \in \nset, \, \EE[|X_t|^2] = \sum_{i=0}^{+\infty} |\theta|^{2i} = \piecewise{
+\infty & |\theta|\geq 1 \\
\frac{1}{1-|\theta|^2} & |\theta| < 1} \]
Ce qui donne $\sup_{t\in \nset} \EE[|X_t|^2] < +\infty \iff |\theta| < 1$ et comme sur $\sup_{t < 0} \EE[|X_t|^2] = 1$ la condition est valable pour le sup sur $t\in \zset$
\end{proof}

\subsubsection{Cas p quelconque}
Pour prouver ce cas, quelques lemmes d'algèbre linéaire sont nécessaires
\begin{Lem}
La matrice $A$ définie dans la preuve de la propriété \ref{prop:cas_simple_unicite} a pour polynôme caractéristique : 
\[
\chi_A (X)= \det(A-XI_p) = (-1)^p \left( X^p - \sum_{i=1}^p \theta_i X^{p-i} \right)
\]
\end{Lem}
\paragraph{Conséquence :}
Les valeurs propres de $A$ sont les inverses des racines de $P = 1- \sum_{i=1}^p \theta_i X^i$
car $\chi_A = (-1)^p X^p P\left( \frac{1}{X}\right)$
\begin{Lem}\label{lem:approx_rayon_spec}
Soit $A \in \cset^{p\times p}$ alors, on rappelle la définition de la norme subordonnée associée à une norme $\norm{.}$ sur $\cset^p$ : \[
\tnorm{A} = \sup_{\norm{x} = 1} \norm{Ax}
\]
On rappelle aussi la définition du rayon spectral $ \rho(A) = \max_{\lambda \in spec(A)} \abs{\lambda}$. On a la propriété suivante : \\
Pour tout $A \in \cset^{p\times p}$, pour tout  $\epsilon >0$ il existe une norme sur $\cset^p$ dépendant de $\epsilon$ et de $A$ telle que la norme subordonnée correspondante $\tnorm{.}_{\epsilon,A}$ vérifie
\[
\tnorm{A}_{\epsilon,A} \leq \rho(A) + \epsilon
\]
\end{Lem}
\begin{Prop}[Condition suffisante]
Dans ce cas particulier, en notant $P(z) = 1 - \sum_{i=1}^p \theta_i z^i$, si $\forall |z| \leq 1\, P(z)\neq 0$ (i.e les racines de $P$ sont hors du disque unité fermé) alors la solution de l'équation \eqref{eq:TVAR} vérifie la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
On part de la définition de $\mb{X}_t$ pour $t \in \nset $ donnée par \eqref{eq:X}

De plus $X_t = \mb{e}_1^T \mb{X}_t$ donc $\EE[|X_t|^2] = \EE[X_t \overline{X}_t^T] = \mb{e}_1^T\EE[\mb{X}_t \mb{X}_t^T] \mb{e}_1$ avec $\forall k \geq t$, comme $\epsilon$ est un bruit blanc
\begin{align*}
\EE[\mb{X}_t \mb{X}_t^T]  
&= \sum_{j=0}^k \sum_{l=0}^k A^j \mb{e}_1 \EE[\epsilon_{t-j} \epsilon_{t-l}] \mb{e}_1^T {(A^l)}^T  + A^{k+1} \EE[ \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix}  {[ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]} ] {(A^{k+1})}^T \\
& = \sum_{j=0}^k A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T + A^{k+1} {(A^{k+1})}^T
\end{align*}
Or on a supposé que les racines de $P$ sont de module strictement supérieur à 1 donc les valeurs propres de $A$ sont de module strictement inférieur à 1. Ainsi $\rho(A) < 1$, il existe donc $\epsilon > 0$ tel que $\rho(A) + \epsilon < 1$. En appliquant le lemme \ref{lem:approx_rayon_spec} à $\epsilon$ et $A$ on obtient (en notant juste $\tnorm{A}$ au lieu de $\tnorm{A}_{A,\epsilon}$ pour alléger les notations) $\tnorm{A} < 1$. \\
Ceci implique dans un premier temps $\tnorm{A^k} \leq \tnorm{A}^k \xrightarrow[k \rightarrow +\infty]{} 0$ donc $A^k \xrightarrow[k \rightarrow +\infty]{} 0$ et ainsi $A^{k+1} (A^{k+1})^T \xrightarrow[k \rightarrow +\infty]{} 0$ \\
De plus $\forall j\in \nset$ $\tnorm{A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T}
\leq \tnorm{A}^{2j} \tnorm{\mb{e}_1 \mb{e}_1^T}$  qui est terme général d'une série convergence dans $\rset$ car $\tnorm{A} < 1$ donc la série des $A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T$ est absolument convergente donc convergente dans $\rset^{p\times p}$. \\
En faisant donc tendre $k$ vers $+\infty$ dans l'expression de $\EE[\mb{X}_t \mb{X_t}^T]$ on obtient : 
\[
\EE[\mb{X}_t \mb{X_t}^T] = \sum_{j=0}^{+\infty} A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T \in \rset^{p\times p}
\]
Ainsi $\EE[|X_t|^2]$ étant le premier coefficient de cette matrice, $\EE[|X_t|^2] < +\infty$
\end{proof}
\subsection{Cas général avec p=1}
L'équation \eqref{eq:TVAR} devient $X_t = \theta(t)X_{t-1} + \sigma(t) \epsilon_t$
\begin{Prop}
Si $\sup_t |\theta(t)| < 1$ et $\sup_t |\sigma(t)| < +\infty$ alors il existe un unique processus $(X_t)_{t \in \zset}$ vérifiant à la fois \eqref{eq:TVAR} et la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
En itérant $k$ fois l'équation on obtient 
\begin{equation}\label{eq:p_1_iter} 
\forall k \geq 0 \, X_t = \left( \prod_{j=0}^{k} \theta(t-j)  \right) X_{t-k-1} + \sum_{j=0}^k \left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)\epsilon_{t-j}
\end{equation}
\begin{itemize}
\item Supposons que $X$ vérifie la condition de stabilité et appelons $M = \sup_t \EE[\abs{X_t}^2]$, $\theta_{\max} = \sup_t |\theta(t)|$ et $\sigma_{\max} = \sup_t |\sigma(t)|$ alors on a $\forall k \geq 0$ :
\[
\norm{\left( \prod_{j=0}^{k} \theta(t-j)  \right) X_{t-k-1} }_2^2 = \left( \prod_{j=0}^{k} \abs{\theta(t-j)}^2 \right) \EE[\abs{X_{t-k-1}}^2] \leq \theta_{\max}^{2(k+1)} M
\]
Ainsi $\lim_{k \rightarrow +\infty} \norm{\left( \prod_{j=0}^{k} \theta(t-j)  \right) X_{t-k-1} }^2 = 0$ car on a pris $\theta_{\max} < 1$.\\ 
De plus $\forall j \geq 0$
\begin{equation}\label{eq:p_1_sommabilite}
\norm{\left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2^2 =  \left( \prod_{i=0}^{j-1} \abs{\theta(t-i)}^2 \right) \abs{\sigma(t-j)}^2 \EE[\abs{\epsilon_{t-j}}^2]
= \left( \prod_{i=0}^{j-1} \abs{\theta(t-i)}^2 \right) \abs{\sigma(t-j)}^2
\leq \theta_{\max}^{2j} \sigma_{\max}^2
\end{equation}
Ce qui donne 
\[
\norm{\left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2 \leq \theta_{\max}^{j} \sigma_{\max}
\]
Encore une fois, parce qu'on a pris $\theta_{\max}<1$, on a à droite de l'inégalité le terme général d'une série convergente. Ainsi en notant $c_j = \left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)$ la série de terme général $c_j \epsilon_{t-j}$ est absolument convergente donc convergente. \\
On peut alors faire tendre $k$ vers $+\infty$ dans \eqref{eq:p_1_iter}, on obtient 
\begin{equation}\label{eq:p_1_solution}
X_t = \sum_{j=0}^{+\infty} c_j  \epsilon_{t-j} \text{ avec } c_j = \left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)
\end{equation}
\item Définissons maintenant $X$ comme dans l'équation \eqref{eq:p_1_solution} et montrons qu'elle vérifie l'équation TVAR et la condition de stabilité. \\
Tout d'abord pour tout $t\in \zset$ on a 
\begin{align*}
\theta(t) X_{t-1} + \sigma(t) \epsilon_t 
&= \theta(t) \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} \theta(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} +\sigma(t) \epsilon_t \\
&= \sum_{j=-1}^{+\infty} \left( \prod_{i=-1}^{j-1} \theta(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} \\
&\underset{(j \leftarrow j+1)}{=}  \sum_{j=0}^{+\infty} \left( \prod_{i=-1}^{j-2} \theta(t-1-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&\underset{(i \leftarrow i+1)}{=} \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} \theta(t-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&= X_t
\end{align*}
\item Remarquons tout d'abord que \eqref{eq:p_1_sommabilite} prouve que $(c_j)_{j\in \nset} \in \ell^2(\nset)$. On a alors $\forall t\in \zset$ 
\begin{align*}
\EE[\abs{X_t}^2] 
&= \EE[\abs{\lim_{k \rightarrow +\infty} \sum_{j=0}^k c_j \epsilon_{t-j}}^2] \\
&= \lim_{k \rightarrow +\infty} \EE[\abs{\sum_{j=0}^k c_j \epsilon_{t-j}}^2] \text{ (par continuité de l'espérance)}\\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \EE[\abs{\epsilon_{t-j}}^2] \text{ (car } \epsilon \text{ est un bruit blanc) } \\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \\
&= \sum_{k=0}^{+\infty}  \abs{c_j}^2
\end{align*}
Ce résultat étant indépendant de $t$ on a bien 
\[
\sup_t \EE[\abs{X_t}^2] = \sum_{k=0}^{+\infty}  \abs{c_j}^2 < +\infty
\]
\end{itemize}

\end{proof}
\subsection{Contre-exemple avec p=2}

On va montrer que la condition obtenue pour le cas $p=1$ n'est plus suffisante pour $p=2$.

Pour ce contre-exemple, on va considérer le TVAR(2) définit comme ceci :
\begin{align*}
X_{2t} = a X_{2t-1} + \epsilon_{2t} \\
X_{2t+1} = b_1 X{2t} + b_2 X_{2t-1} + \epsilon_{2t+1}
\end{align*}

pour $t>0$ sinon $X_t = \epsilon_t$.

On a alors :
$$
\forall t>0, X_{2t+1} = (ab_1 + b_2) X_{2t-1} + b_1 \epsilon_{2t} + \epsilon_{2t+1}
$$
On a alors que le processus $Y_t = X_{2t+1}$ suit l'équation d'un AR(1) dont la condition de stabilité implique :
\begin{equation} \label{eq:racines}
|ab_1 + b_2| < 1
\tag{*}
\end{equation}

D'autre part, le polynôme caractéristique associé est $P(z) = 1 - b_1 z - b_2 z^2$. Posons $P(z) = (1-bz)^2$. On a alors $b_1 = 2b$ et $b_2 = -b^2$.
La condition \eqref{eq:racines} donne:
$$
|2ba-b^2| <1
$$
Avec $a = -b$, celle-ci devient :
$$
3b^2 < 1
$$
Or ceci peut être faux. Il suffit de prendre $b=\frac{1}{\sqrt{2}}$ par exemple.

On a donc trouvé une sous-suite $Y_t$ du processus $X_t$ qui diverge. Donc la condition initiale, à savoir $\sup_t|\theta_i(t)|<1$, n'est plus suffisante pour assurer la stabilité d'un TVAR(2).

\subsection{Cas général}
On introduit une nouvelle définition du modèle TVAR introduite dans \citep{Dahlhaus:1996} :
\begin{Def} \label{def:TVAR}
Soient $p\geq 1$, $\theta_1,\cdots, \theta_p$ et $\sigma$ des fonctions définies sur $]-\infty,1]$ et $(\epsilon_t)_{t\in \zset}$ une suite i.i.d de variables aléatoires avec moyenne nulle et variance unitaire. Pour tout $T \geq 1$ on dit que $(X_{t,T})_{t\leq T}$ est un processus TVAR stable s'il vérifie les deux conditions suivantes 
\begin{description}
\item[(i)] $\forall -\infty < t \leq T $
\begin{equation}\label{eq:TVAR'}
X_{t,T} = \sum_{i=1}^p \theta_i\left( \frac{t}{T}\right) X_{t-i,T} + \sigma\left( \frac{t}{T} \right) \epsilon_t
\tag{TVAR'}
\end{equation}
\item[(ii)]
\begin{equation}\label{eq:S'}
\sup_{-\infty < t\leq T} \EE[\abs{X_{t,T}}^2] < +\infty
\tag{S'}
\end{equation}
\end{description}
\end{Def}
\begin{Rque}
Revenons sur cette nouvelle définition du modèle TVAR. Supposons $u \in ]-\infty , 1]$ fixé et $T \gg 1$. Si on veut étudier ce qu'il se passe pour $t$ dans un intervalle du type $[(u-b)T,(u+b)T ]$, alors on peut prendre b petit sans avoir un intervalle trop petit pour $t$. Alors $\frac{t}{T}\in [u-b,u+b]$ reste proche de u et si $\theta_i$ et $\sigma$ sont suffisamment régulières, on peut approcher les approcher par des constantes. Ainsi l'équation s'approche par une équation AR.

\end{Rque}
\begin{Prop}\label{prop:TVAR}
Supposons que les coefficients $\theta_i$ de l'équation \eqref{eq:TVAR} sont uniformément continus sur $]-\infty,1]$ et que $\sigma$ est bornée sur $]-\infty,1]$. Supposons de plus qu'il existe $\delta \in ]0,1[$ tel que $\Theta(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]$ où 
\[
\Theta(z;u) = 1 - \sum_{i=1}^p \theta_i(u) z^i
\]
Alors il existe $T_0 \geq 1$ tel que $\forall T \geq T_0$ il existe un unique processus $(X_{t,T})_{t\leq T}$ vérifiant \eqref{eq:TVAR'} et \eqref{eq:S'}. De plus cette solution s'écrit : 
\begin{equation}\label{eq:repr_lineaire}
\forall -\infty < t \leq T
 \,\, X_{t,T} = \sum_{j=0}^{+\infty} \phi_{t,T} (j) \sigma\left( \frac{t-j}{T} \right) \epsilon_{t-j}
\end{equation}
avec $\forall \delta_1 \in ]\delta, 1[$
$$
\sup_{T \geq T_0} \sup_{-\infty < t \leq T} \sup_{j\geq 0} \delta_1^{-1} \abs{\phi_{t,T}(j)} < +\infty
$$

\end{Prop}
\begin{proof}
La preuve de cette proposition peut être trouvée dans \citep{giraud-roueff-sanchez-aos2015}
\end{proof}
\section{Prédiction sur un TVAR}
\paragraph{Détail pratique pour l'implémentation :} Dans cette partie, nous considèrerons que les coefficients sont constantes pour $u \leq 0$ : $\theta_i(u) = \theta_i(0)$ si $u \leq 0$. Dans la pratique nous prendrons $0$. Nous allons donc nous restreindre à $[0,1]$
\subsection{Implémentation d'un TVAR}
\paragraph{Coefficients AR abstraits associés}
Il va être intéressant pour la suite d'adopter la vision suivante : \\
On se donne $p$ fonctions $\theta_1, \cdots, \theta_p$ vérifiant les hypothèses de la proposition \ref{prop:TVAR}. Alors à $T$ donné et $-\infty < t \leq T$ fixé si on note $u = \frac{t}{T}$ les coefficients $(\theta_1(u), \cdots , \theta_p(u))$ sont ceux d'un AR(p) causal car $\Theta(z;u) \neq 0 \,\, \forall |z| \leq 1$.\\
On peut donc à $t$ fixé se ramener à ce que l'on connaît sur les processus AR en considérant un processus AR(p) abstrait associé à ces coefficients (notamment utiliser l'algorithme de Levinson-Durbin on considérer la densité spectrale de puissance). Il y un processus AR(p) abstrait local à chaque temps $t$ et notamment une densité spectrale de puissance locale à chaque instant $t$. \\

Maintenant que nous considérons à chaque instant $u$ un processus AR(p) local nous pouvons appliquer l'algorithme \ref{algo:construction} et le théorème \ref{thm:kappa} pour chaque instant $u$ à partir de $p$ fonctions continues $\kappa_i :[0,1] \to \osego{-1,1}$ pour $i=1,\cdots,p$
\begin{Thm}
\label{thm:kappa_u}
Quelles que soient les fonctions $\kappa_1, \cdots, \kappa_p$ continues sur $[0,1]$ et à valeurs dans $\osego{-1,1}$, les fonctions $\theta_{1,p},\cdots, \theta_{p,p}$ obtenues en appliquant l'algorithme \ref{algo:construction} à chaque instant $u$ sont les coefficients d'un processus TVAR(p) stable i.e elles sont uniformément continues sur $[0,1]$ et il existe $\delta \in ]0,1[$ tel que $\Theta(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]$ où 
\[
\Theta(z;u) = 1 - \sum_{i=1}^p \theta_{i,p}(u) z^i
\]
\end{Thm}
\begin{proof} 
Tout d'abord les $(\theta_{i,p})_{i\in \iseg{1,p}}$ sont obtenus par sommes et produits des $(\kappa_i)_{i\in \iseg{1,p}}$ donc sont continus sur $[0,1]$ donc uniformément continus sur ce segment. \\
Supposons maintenant $\forall \delta \in ]0,1[, \exists \abs{z} < \delta^{-1}, u\in [0,1], \Theta(z;u) = 0$. Considérons une suite $(\delta_n)_{n\in \nset} \in ]0,1[^\nset$ telle que $\delta_n \uparrow 1$. Alors il existe une suite $(z_n)_{n\in \nset}$ et une suite $(u_n)_{n\in \nset}$ telle que $\forall n \in \nset$
$$
\abs{z_n} < \delta_n^{-1},
\,\,
u_n \in [0,1] 
\mbox{ et }
\Theta(z_n;u_n) = 0
$$
On peut alors prendre une sous suite $(z_{\alpha_n}, u_{\alpha_n})_{n\in \nset}$ de $(z_n,u_n)_{n\in \nset}$ qui converge vers un couple $(z,u)$ tel que $\abs{z} \leq 1$ et $u\in [0,1]$. On a en effet $\abs{z} \leq 1$ car $\forall n\in \nset, \abs{z_{\alpha_n}} < \delta_{\alpha_n}^{-1}$ et par passage à la limite $\abs{z} \leq 1$ \\
De plus par continuité de $\Theta$, en passant à la limite dans $\forall n\in \nset, \Theta(z_{\alpha_n}, u_{\alpha_n}) = 0$ on obtient finalement :
$$
\abs{z} \leq 1,
\,\,
u \in [0,1] 
\mbox{ et }
\Theta(z;u) = 0
$$
Ce qui contredit le résultat du théorème \ref{thm:kappa} appliqué à $\kappa_1(u),\cdots,\kappa_p(u)$ qui dit que $\Theta(z;u) \neq 0, \forall \abs{z}\leq 1$
\end{proof}

\subsection{Définition du prédicteur TVAR}
On considère un processus TVAR $X_{t,T}$ définit grâce à la proposition \ref{prop:TVAR}. Notons que l'équation \eqref{eq:TVAR'} se réécrit de manière plus compacte par : $\forall t \in \iseg{1,T}$
$$
X_{t,n} = \bftheta_{t-1,T}^T \mb{X}_{t-1,T} + \sigma_{t,T} \epsilon_{t,T}
$$
où 
\begin{align*}
&\mb{X}_{t,T} = [X_{t,T}, X_{t-1,T}, \cdots , X_{t-p+1,T} ]^T \\
&\bftheta_{t,T} = \bftheta\left(\frac{t}{T}\right) = \left[ \theta_1\left(\frac{t}{T}\right), \theta_2\left(\frac{t}{T}\right), \cdots, \theta_p\left(\frac{t}{T}\right) \right]^T \\
&\sigma_{t,T} = \sigma\left(\frac{t}{T}\right)
\end{align*}
On cherche un estimateur de $\bftheta_{t,T}$ définit par 
$$
\hat{\bftheta}_{t,T} = \argmin_{\bftheta \in \cset^p} \EE[ \abs{X_{t+1,T} - \bftheta^T \mb{X}_{t,T}}^2]
$$
On définit cet estimateur en ligne par l'algorithme des moindres carrés normalisé (NLMS):
\begin{Def}[Estimateur NLMS de $\bftheta$]
On se donne un pas $\mu$, l'estimateur est le suivant :
\begin{equation}\label{eq:NLSM}
\begin{array}{l}
\hat{\bftheta}_{0,T}(\mu) = 0 \\
\forall t \in \iseg{0,T-1}, \, \hat{\bftheta}_{t+1,T}(\mu) = \hat{\bftheta}_{t,T}(\mu) + \mu ( X_{t+1,T} - \hat{\bftheta}_{t+1,T}(\mu)^T \mb{X}_{t,T} ) \frac{\mb{X}_{t,T}}{1+\mu \norm{\mb{X}_{t,T}}^2}
\end{array}
\tag{NLMS}
\end{equation}
On définit ensuite l'estimateur de $\bftheta : u \rightarrow \bftheta(u) = \left[ \theta_1(u), \theta_2(u), \cdots, \theta_p(u) \right]^T$ pour $u\in [0,1]$ en faisant une interpolation :
$$
\hat{\bftheta}_T(u ; \mu) = \hat{\bftheta}_{\floor{uT},T}(\mu)
$$
A un instant $u$, l'estimateur est une fonction de $\mb{X}_{0,T}, X_{1,T}, \cdots , X_{\floor{uT},T}$ et $\mu$
\end{Def}
\begin{Def}[Prédicteur associé]
On définit à partir de $(\bftheta_{t,T}(\mu))_{1\leq t \leq T}$ le prédicteur suivant :
$\forall 1 \leq t \leq T$
\begin{equation}\label{eq:predictor}
\hat{X}_{t,T}(\mu) = \hat{\bftheta}_{t-1,T}(\mu)^T \mb{X}_{t-1,T}
\end{equation}
\end{Def}
\begin{Def}[$\beta$-Lipschitz semi-norme]
Soit $\beta \in ]0,1]$ et $\mb{f} : [0,1] \to \rset^d$, on définit la semi-norme $\beta$-Lipschitz de $f$ par :
$$
\abs{\mb{f}}_{\Lambda,\beta} = \sup_{t\neq s} \frac{\norm{\mb{f}(t)-\mb{f}(s)}}{\abs{t-s}^\beta}
$$
On définit la boule $\beta$-Lipschitz de rayon $L > 0$ : 
$$
\Lambda_d(\beta,L) = \ens{ \mb{f} : [0,1] \to \rset^d, \abs{\mb{f}}_{\Lambda,\beta} \leq L, \sup_{t\in [0,1]} \norm{\mb{f}(t)} \leq L }
$$
\end{Def}
\paragraph{Notations :}
On notera pour $Z$ v.a à valeur dans un espace muni d'une norme $\norm{.}$ : $\norm{Z}_q = (\EE[\norm{Z}])^{1/q}$ et  $\norm{Z}_{q,\bftheta,\sigma} = (\EE_{\bftheta,\sigma}[\norm{Z}])^{1/q}$ dans le cas où l'on a une paramétrisation par $(\bftheta,\sigma)$ \\
On note de plus 
\begin{itemize}
\item pour $\rho > 0$ : 
$$
S(\rho) = \ens{\bftheta = [\theta_1, \cdots , \theta_p]^T : [0,1] \to \rset^p, \Theta(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]}
$$
\item pour $\beta \in ]0,1], L >0, \rho \in ]0,1[, 0 < \sigma_- \leq \sigma_+ < +\infty $ : 
$$
C(\beta, L, \rho, \sigma_-, \sigma_+) = \ens{(\bftheta,\sigma), \bftheta \in \Lambda_d(\beta,L) \cap S(\rho), \sigma : [0,1] \to [\sigma_-, \sigma_+]}
$$
\end{itemize}

\begin{Thm}
On suppose que $\epsilon$ est un bruit blanc centré de variance unitaire indépendant de $\mb{X}_{0,T}$ et de plus que $\sup_{T \geq 1} \norm{\mb{X}_{0,T}}_q < +\infty$ et $\sup_{1\leq t \leq T} \norm{\epsilon_{t,T}}_q < +\infty$ avec $q \geq 4$. \\
Soient $p\in [1,q/3[, \beta\in ]0,1], L>0, \rho \in ]0,1[$ et $0 < \sigma_{-} \leq \sigma_{+}$ \\
Alors $\exists M,\delta > 0$ et $\mu_0 >0$ tels que $\forall \mu \in ]0,\mu_0], T \geq 1, u\in ]0,1]$ et $(\bftheta, \sigma) \in C(\beta, L, \rho, \sigma_-, \sigma_+)$ 
$$
\norm{\hat{\bftheta}_T(u ; \mu) - \bftheta(u)}_{p,\bftheta,\sigma} \leq M \left( \norm{\bftheta(0)}(1-\delta \mu)^{uT} + \sqrt{\mu} + (T\mu)^{-\beta} \right)
$$
\end{Thm}
\begin{proof}
Le preuve peut être trouvée dans \citep{moulines-priouret-roueff-2005}
\end{proof}
\begin{Rque}
Cette borne supérieure peut s'interpréter de la manière suivante :
\begin{itemize}
\item Le premier terme $\norm{\bftheta(0)}(1-\delta \mu)^{uT}$ traduit l'oubli de l'erreur initiale de l'estimateur quand $T$ augmente
\item Le deuxième terme $\sqrt{\mu}$ est un terme de variance de l'estimateur qui serait là même si $u \mapsto \bftheta(u)$ était constante
\item Le dernier terme $(T\mu)^{-\beta}$ contrôle l'erreur due à l'évolution de $\bftheta$ dans le temps. Elle décroit d'autant plus vite que $\beta$ est grand, ce qui implique une bonne régularité de $\bftheta$
\end{itemize}
\end{Rque}
\subsection{Analyse en densité spectrale de puissance}
Comme expliqué précédemment, on considère pour un processus TVAR donné, les processus AR abstraits à chaque instant $u\in [0,1]$, on définit ainsi les DSP "locales" comme les DSP associées à chaque AR.
\begin{Def}[DSP]
La DSP locale théorique en $u\in [0,1]$ est définie pour $\lambda \in [-\pi, \pi[$ 
$$
S_x(\lambda ; u) = \frac{\sigma^2(u)}{2\pi\abs{\Theta\left( e^{-i\lambda} ; u \right)}^2}
$$
où $\Theta(z;u) = 1 - \sum_{k=1}^p \theta_k(u) z^k$ \\
La DSP locale estimée est : 
$$
\hat{S}_x(\lambda ; u) = \frac{\hat{\sigma}^2(u)}{2\pi\abs{\hat{\Theta}\left( e^{-i\lambda} ; u \right)}^2}
$$
où $\hat{\Theta}(z;u) = 1 - \sum_{k=1}^p \hat{\theta}_k(u) z^k$
\end{Def}

En appliquant la proposition \ref{prop:racines_phase}, à chaque instant les modules des inverses racines sont assez proches de $1$, les pics seront proches des phases des inverses des racines.
\section{Agrégation des prédicteurs}
\subsection{Algorithme et stratégies}
On applique dans cette section les méthodes d'agrégation en ligne fournies dans \citep{giraud-roueff-sanchez-aos2015} au cadre des processus TVAR. On se donne des observations NLMS d'un processus sur une période de temps $\ens{1,\cdots,T}$ pour différentes valeur de $\mu$, le pas de descente de gradient :  $(X_{t,T})_{1 \leq t \leq T}$ vérifiant \eqref{eq:TVAR'} et \eqref{eq:S'}. On considère $N$ prédicteurs définis par \eqref{eq:predictor} associés à $N$ pas $\mu_1, \cdots, \mu_N$ : on notera $\forall i \in \iseg{1,N}, \forall 1\leq t \leq T, \hat{X}_{t,T}^{(i)} = \hat{X}_{t,T}(\mu_i)$
\begin{Def}[Agrégation]
Agréger les prédicteurs $\ens{\left(\hat{X}_{t,T}^{(i)}\right)_{1\leq t\leq T}, i\in \iseg{1,N}}$ signifie pour tout $1\leq t \leq T$
\begin{itemize}
\item Choisir $\alpha_t=(\alpha_{1,t},\cdots,\alpha_{N,t}) \in \rset_+^N$ tel que $\sum_{i=1}^N \alpha_{i,t} = 1$ 
\item Calculer le nouvel estimateur $\hat{X}_{t,T} = \hat{X}_{t,T}^{[\alpha_t]} = \sum_{i=1}^N \alpha_{i,t} \hat{X}_{t,T}^{(i)}$
\end{itemize}
\end{Def}
Comme expliqué dans \citep{giraud-roueff-sanchez-aos2015}, on peut approcher les poids d'agrégation optimaux $\alpha_t$ en calculant en ligne et récursivement des poids $\hat{\alpha}_t$ selon deux stratégies à partir d'un paramètre $\eta >0$ nommé "pas d'apprentissage". \\
\paragraph{Stratégie 1 : à partir du gradient de l'erreur quadratique}
$\forall i \in \iseg{1,N}, \forall t\in \iseg{1,T}$
$$
\hat{\alpha}_{i,t} = \frac{\exp\left( -2\eta \sum_{s=1}^{t-1} \left( \sum_{j=1}^N \hat{\alpha}_{j,s} \hat{X}_{s,T}^{(j)}-X_{s,T} \right)\hat{X}_{s,T}^{(i)}\right)}{\sum_{k=1}^N \exp\left( -2\eta \sum_{s=1}^{t-1} \left( \sum_{j=1}^N \hat{\alpha}_{j,s} \hat{X}_{s,T}^{(j)}-X_{s,T} \right)\hat{X}_{s,T}^{(k)}\right)}
$$
\paragraph{Stratégie 2 : à partir de l'erreur quadratique}
$\forall i \in \iseg{1,N}, \forall t\in \iseg{1,T}$
$$
\hat{\alpha}_{i,t} = \frac{\exp\left( -\eta \sum_{s=1}^{t-1} \left( \hat{X}_{s,T}^{(i)} - X_{s,T}\right)^2\right)}{\sum_{k=1}^N \exp\left( -\eta \sum_{s=1}^{t-1} \left( \hat{X}_{s,T}^{(k)} - X_{s,T}\right)^2\right)}
$$
Dans les deux cas on prend la convention qu'une somme sur aucun élément est nulle donc $\forall i \in \iseg{1,N}, \hat{\alpha}_{i,1} = 1/N$. 
\begin{Rque}
Les deux stratégies sont bien en ligne et récursives ce qui signifie 
\begin{itemize}
\item Pour calculer $\alpha_t$ on n'a besoin que des valeurs des $\ens{ \hat{X}_{s,T}^{(i)}, i\in \iseg{1,N}}$ pour $s\leq t-1$
\item A chaque étape peut calculer $\hat{\alpha}_t$ à partir de $\hat{\alpha}_{t-1}$
\end{itemize}
\end{Rque}
La construction du prédicteur à partir de ces stratégies est fournit par l'algorithme suivant : \\
\begin{algorithm}[H]
 \KwData{$\eta > 0$, strategy (1 ou 2), les prédicteurs $\hat{X}_{t,T}^{(i)}$, les observations $X_{t,T}$}
 \KwResult{$\hat{X}_{t,T}=\hat{X}_{t,T}^{[\hat{\alpha}_t]}$ pour $t=1,\cdots,T$ }
 \KwInit{
 	$\hat{\alpha}_1 = [1/N,\cdots,1/N]$\;
	$\hat{X}_{1,T} = \sum_{i=1}^N \hat{\alpha}_{i,1} \hat{X}_{1,T}^{(i)}$\;  
 }
 \For{$t=2,\cdots,T$}{
 	\For{$i=1, \cdots, N$}{
 		\Switch{strategy}{
 			\Case{1}{
				$v_{i,t} = \hat{\alpha}_{i,t-1} \exp\left( -2\eta (\hat{X}_{t-1,T} - X_{t-1,T}) \hat{X}_{t-1,T}^{(i)}\right)	$		
 			}
 			\Case{2}{
				$v_{i,t} = \hat{\alpha}_{i,t-1} \exp\left( -\eta (\hat{X}_{t-1,T}^{(i)} - X_{t-1,T})^2 \right)	$		
 			}
 		}
 		$\hat{\alpha}_t = \left( \frac{v_{i,t}}{\sum_{k=1}^N v_{k,t}} \right)_{i=1,\cdots,N}$
 	}
 	$\hat{X}_{t,T} = \sum_{i=1}^N \hat{\alpha}_{i,t} \hat{X}_{t,T}^{(i)}$
 
 }
 \caption{Agrégation des prédicteurs}
 \label{algo:agregation}
\end{algorithm}

Le choix de la stratégie pour mettre à jour les coefficients $\alpha^{i}$ a une importance sur le prédicteur que l'on souhaite atteindre. En effet la borne de l'erreur n'est pas la même dans les deux cas. Les preuves de ces bornes se trouvent dans \citep{giraud-roueff-sanchez-aos2015}.\\
\paragraph{Stratégie 1 : Gradient de l'erreur quadratique}
La stratégie fait intervenir le gradient de l'erreur quadratique et permet d'approcher la meilleure combinaison convexe dans le simplexe $S_N$. \citep{giraud-roueff-sanchez-aos2015} donne la relation suivante :
\begin{equation}
\frac{1}{T} \sum_{t=1}^T \mathbb{E}[(\hat X_t-X_t)^2] \leq \inf_{\alpha \in S_N} \frac{1}{T} \sum_{t=1}^T \mathbb{E}[(\hat X_t^{[\alpha]}-X_t)^2] + \frac{\log(N)}{T \eta} + \beta_{\eta}
\end{equation}

Où $\beta_{\eta}$ contrôle les variations du modèle des prédicteurs choisis et $\hat X_t^{[\alpha]}$ est le prédicteur par agrégation obtenu lorsque l'on choisit la combinaison convexe $\alpha$ à l'instant $t$. \\

Donc pour $T{\longrightarrow}\infty$, on a bien que le prédicteur obtenu par la stratégie 1 converge bien en moyenne vers le meilleur des prédicteurs par agrégation (on a en fait $\hat\alpha\underset{T\longrightarrow\infty}{\longrightarrow}\alpha$).


\paragraph{Stratégie 2 : Erreur quadratique}
Cette stratégie permet d'obtenir elle aussi un prédicteur par agrégation mais cette fois on ne peut espérer faire mieux que le meilleur des estimateurs donnés en entré de l'algorithme. \citep{giraud-roueff-sanchez-aos2015} donne cette fois la borne :
\begin{equation}
\frac{1}{T} \sum_{t=1}^T \mathbb{E}[(\hat X_t-X_t)^2] \leq \min_{1 \leq i \leq N} \frac{1}{T} \sum_{t=1}^T \mathbb{E}[(\hat X_t^{(i)}-X_t)^2] + \frac{\log(N)}{T \eta} + \beta_{\eta}
\end{equation}

Où $X_t^{(i)}$ est le i-ème prédicteur à l'instant $t$.
Cette fois pour $T{\longrightarrow}\infty$, on ne va donc ne trouver que le meilleur des prédicteurs dont on dispose.

\begin{Rque}
Le choix de $\eta$ aussi appelé "taux d'apprentissage" ou encore "learning rate" a à voir avec le bruit dont sont entachées les observations. D'après  \citep{giraud-roueff-sanchez-aos2015}, on peut intuitivement se dire que le bruit dégrade le risque empirique et $\eta$ permet de quantifier l'importance du risque empirique dans la décision de mise à jour des coefficients $\alpha^{i}$. Donc plus le bruit est important et dégrade la qualité des observation et moins on doit se fier au risque empirique (ie : $\eta {\rightarrow} 0$).
\end{Rque}

\begin{Rque}
La stratégie 1 donne un meilleur résultat théorique que la stratégie 2 mais les conditions d'application de la première stratégie sont plus contraignante alors que la seconde est applicable à n'importe quel ensemble d'échantillons.
\end{Rque}

\subsection{Comparaison des risques d'estimation et d'agrégation}
On se propose de montrer dans cette sous-section que le risque empirique d'un prédicteur par agrégation (quelque soit la stratégie) est moindre que celui d'un estimateur obtenu classiquement par NLMS.
\begin{Prop}
Soit $(X_t)_{t \in \mathbb{Z}}$ un processus TVAR(d) avec d dans $\mathbb{N}^*$. Soit $(\theta_t)_{t \in \mathbb{Z}}$ la famille de filtres associé tels que $\forall t \in [0,T]$, $X_t = \sum_{k=1}^d \theta_k X_{t-k}$.\\
On pose $(\hat X_t)_{t \in \mathbb{Z}}$ et $(\hat\theta_t)_{t \in \mathbb{Z}}$ les estimateurs associés. Alors $\forall t$: \\
\begin{equation}
\mathbb{E}[(\hat X_t - X_t)^2] \leq \mathbb{E}[||\hat \theta_t - \theta_t||^2] \mathbb{E}[||X_{t-1}||^4]^{1/2}
\end{equation}
\end{Prop}
\begin{proof}
Partons du risque empirique pour une observation et son estimation au temps $t$:
\begin{equation}
\mathbb{E}[(\hat X_t - X_t)^2] = \mathbb{E}[((\hat\theta_t - \theta) X_{t-1} - \sigma(t-1))^2]
\end{equation}
\begin{equation}
\mathbb{E}[(\hat X_t - X_t)^2] = \mathbb{E}[((\hat\theta_t - \theta) X_{t-1})^2] + \sigma(t-1))^2
\end{equation}
On peut alors appliquer l'inégalité de Cauchy-Schwarz à cette expression :
\begin{equation}
\mathbb{E}[(\hat X_t - X_t)^2 \leq \mathbb{E}[||\hat\theta_t - \theta||^4]^{\frac{1}{2}} \mathbb{E}[||X_{t-1}||^4]^{\frac{1}{2}}
\end{equation}
Or par l'inégalité de Holder, on peut montrer facilement que si $f$ est mesurable et $f \in L^4$, alors $f \in L^2$. D'où l'expression voulue.
\end{proof}
\chapter{Implémentations}
\section{Etude spectrale sur un TVAR(1) et un TVAR(2) à partir des racines}
\subsection{Cas d'un TVAR(1)}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document} 
