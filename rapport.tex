\documentclass{report}
\usepackage[utf8]{inputenc}
\RequirePackage[numbers]{natbib}
\input{defs.sty}
\input{init.sty}
\title{SIGMA205 : Prédiction d'une série temporelle localement
stationnaire}
\author{Thomas EBOLI et Amaury DURAND}

\begin{document}
\maketitle
\tableofcontents
\pagebreak
%\chapter*{Notations}
%\pagebreak
\chapter{Rappels sur les processus AR}
\paragraph{Equation AR(p) :} 
On s'intéresse à l'équation AR(p) définie pour $p\geq 1$ et $(X_t)_{t\in \zset}$ un processus aléatoire à valeurs dans $\cset$ par 
\begin{equation}\label{eq:AR}
X_t = \sum_{i=1}^p \theta_i X_{t-i} + \epsilon_t
\tag{AR}
\end{equation}
où $\theta_1,\cdots,\theta_p \in \cset$ et $\epsilon \sim BB(0,\sigma^2)$ (i.e $\epsilon$ est un processus stationnaire au second ordre, centré de fonction d'autocovariance $\gamma$ vérifiant $\gamma(0)=\sigma^2 > 0$ et $\forall s\neq 0, \gamma(s)=0$)
\section{Construction d'une solution stationnaire au second ordre}
\begin{Thm}
Soit $P(z)=1-\sum_{k=1}^p \theta_k z^k$, supposons que $\forall |z|=1, P(z)\neq 0$ \\
Alors il existe une solution stationnaire au second ordre de \eqref{eq:AR} 
\end{Thm}
\begin{proof}
Pour prouver cela on a besoin du lemme suivant :
\begin{Lem}
Pour $\alpha \in \ell^1$ et $X$ un processus tel que $\sup_t \EE[|X_t|] < +\infty$ on appelle filtrage de $X$ le processus : 
\[ F_\alpha(X) = \left( \sum_{k\in \zset} \alpha_k X_{t-k} \right)_{t\in \zset} \]
Alors si $\alpha,\beta \in \ell^1$ on a $F_\alpha (F_\beta (X)) = X$ si $\alpha \star \beta = \delta$ \\
De plus $\alpha \star \beta = \delta \iff \forall |z|=1 \, \left( \sum_{z\in \zset} \alpha_k z^k \right) \left( \sum_{z\in \zset} \beta_k z^k \right) =1 $ \\
Enfin si $X$ est stationnaire au second ordre $F(X)$ l'est aussi.
\end{Lem}
\begin{Rque}
$F_\alpha = \sum_{k\in \zset} \alpha_i B^k$ où $B$ est l'opérateur de shift $ B : (x_t)_{t\in \zset} \mapsto (x_{t-1})_{t\in \zset}$
\end{Rque}

Le polynôme $P$ peut s'écrire : $P(z)=\prod_{k=1}^p (1-u_k z)$ où les $u_k$ sont les inverses des racines de $P$. L'équation \eqref{eq:AR} s'écrit $P(B)(X)=\epsilon$. Or avec la factorisation obtenue on a : 
\[
P(B) = (1-u_1 B) \circ \cdots \circ (1-u_p B) = F_{\alpha^{(1)}} \circ \cdots \circ F_{\alpha^{(p)}}
\]
où $\alpha_k^{(l)} = \piecewise{
1 & k=0 \\
-u_l & k=1 \\
0 & \text{sinon} \\}
$ \\
Le but désormais est de chercher pour tout $l \in \llbracket 1, p \rrbracket$ un $\beta^{(l)}$ tel que $\alpha^{(l)} \star \beta^{(l)} = \delta$ pour pouvoir inverser la relation. On sait d'après la remarque qu'il suffit de trouver $\beta^{(l)}$ tel que $\frac{1}{1-u_l z} = \sum_{k \in \zset} \beta_k z^k$ pour tout $|z|=1$ \\
On sait de plus que $\forall l \in \llbracket 1, p \rrbracket$ $|u_l| \neq 1$ par hypothèse sur les racines de $P$.\\
Prenons $l\in \llbracket 1,p \rrbracket$ alors deux cas sont possibles :
\begin{itemize}
\item si $|u_l|< 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} = \sum_{k \geq 0} u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
u_l^k & k \geq 0 \\
0 & \text{sinon} \\
}$
\item si $|u_l|> 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} \frac{-u_l^{-1} z^{-1}}{1-u_l^{-1} z^{-1}} = \sum_{k \leq -1} -u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
-u_l^k & k \leq -1 \\
0 & \text{sinon} \\
}$
\end{itemize}
Finalement prenons $F = F_{\beta^{(p)}} \circ \cdots \circ F_{\beta^{(1)}}$, on prends alors 
\[
X = F(\epsilon) 
\]
\end{proof}

\section{Prédiction}
\begin{Prop}
On se donne $X$ stationnaire au second ordre vérifiant \eqref{eq:AR}. On suppose de plus que $X$ est causal i.e $P(z) \neq 0$ pour tout $|z| \leq 1$
On note $\mathcal{H}_t^X = \overline{\mathrm{Vect}\ens{X_s, s\leq t}}$. Alors
\[ \hat{X}_{t+1} = proj(X_{t+1} | \mathcal{H}_t^X ) = \sum_{k=1}^p \theta_k X_{t+1-k} \]
\end{Prop}
\begin{proof}
$X_{t+1} = \sum_{k=1}^p \theta_k X_{t+1-k} +\epsilon_{t+1}$ de plus $\forall k \in \llbracket 1,p \rrbracket \, X_{t+1-k} \in \mathcal{H}_t^X$ donc
\[ \hat{X}_{t+1} = \sum_{k=1}^p \theta_k X_{t+1-k} + proj(\epsilon_{t+1} | \mathcal{H}_t^X ) \]
Or $X$ est causal donc dans la construction de $X$ (cf preuve d'avant) on a $P(z)= \prod_{l=1}^p (1-u_l z)$ où $\forall l |u_l|<1$ ainsi les $\beta^{(l)}$ correspondants sont tous à support dans $\nset$ ce qui entraine que $\psi = \beta^{(p)} \star \cdots \star \beta^{(1)}$ est aussi à support dans $\nset$ et donc 
\[ X_t = \sum_{k=0}^{+\infty} \psi_k \epsilon_{t-k}\] On en déduit que $\mathcal{H}_t^X = \mathcal{H}_t^\epsilon$ et donc comme $\epsilon$ est un bruit blanc $proj(\epsilon_{t+1} | \mathcal{H}_t^X ) = proj(\epsilon_{t+1} | \mathcal{H}_t^\epsilon ) = 0$ d'où la solution
\end{proof}

\section{Algorithme de Levinson-Durbin}
On rappelle ici l'algorithme de Levinson Durbin et quelques conséquences. Le cadre d'étude est le suivant : \\
On se donne un processus stationnaire au second ordre $X$ de fonction d'autocovariance $\gamma$. On suppose que pour tout $p\geq 1$ la matrice de covariance de $(X_{t-1}, \cdots , X_{t-p})$ $\Gamma_p$ est inversible. Cette matrice est la matrice de Toeplitz associées à $(\gamma(0),\cdots, \gamma(p-1))$.  \\
On note pour tout $p\geq 1$, pour tout $k\in \iseg{1,p}$, $\theta_{k,p}$ les coefficients de prédiction direct d'ordre $p$ de $X_t$ à partir de son passé et $\sigma_p^2$ l'erreur de prédiction i.e 
$$
proj(X_t | \calH_{t-1,p}^X)= \sum_{k=1}^p \theta_{k,p} X_{t-k} 
\mbox{ et } 
\sigma_p^2 = \norm{X_t - proj(X_t | \calH_{t-1,p}^X)}^2
$$
où $\calH_{t-1,p}^X = \vect(X_{t-1}, \cdots, X_{t-p})$ \\
L'algorithme de Levinson permet de trouver $(\theta_{1,p+1}, \cdots, \theta_{p,p})$ connaissant $(\theta_{1,p-1}, \cdots, \theta_{p-1,p-1})$\\
\begin{algorithm}[H]
 \KwData{$\gamma(1),\cdots,\gamma(d)$}
 \KwResult{$(\theta_{i,p})_{p\in \iseg{1,d},i\in \iseg{1,p}}$ }
 \KwInit{ $\kappa_1 = \frac{\gamma(1)}{\gamma(0)}$, $\theta_{1,1} = \kappa_1$, $\sigma_1^2 = \gamma(0)(1-\kappa_1^2)$\;}
 \For{$p = 2,\cdots,d$}{
 	$\kappa_p = \sigma_{p-1}^{-2} \left( \gamma(p) - \sum_{k=1}^{p-1} \theta_{k,p-1} \gamma(p-k) \right)$ \;	
   $\theta_{p,p} = \kappa_p$ \;
   $\sigma_p^2 = \sigma_{p-1}^2 (1-\kappa_p^2)$ \;
 	\For{$m = 1\cdots p-1$}{
 	$\theta_{m,p} = \theta_{m,p-1} - \kappa_p \theta_{p-m,p-1}$
 	}
 }
 \caption{Levinson-Durbin}
 \label{algo:levinson}
\end{algorithm}
\begin{Prop}
Les coefficient $\kappa_p$ s'appellent coefficients d'autocorrélation partielle et vérifient :
$$
\forall p\geq 1, \abs{\kappa_p} \leq 1
$$
\end{Prop}
On peut alors en générant une famille $(\kappa_1, \cdots , \kappa_d) \in \seg{-1,1}^d$ calculer des coefficients $\theta$ en inversant les équations de Levinson-Durbin grâce à l'algorithme suivant \\
\begin{algorithm}[H]
 \KwData{$(\kappa_1, \cdots , \kappa_d) \in \seg{-1,1}^d$}
 \KwResult{$(\theta_{i,p})_{p\in \iseg{1,d},i\in \iseg{1,p}}$ }
 \KwInit{ 
 	\For{$p = 1, \cdots, d$}{
 	$\theta_{p,p} = \kappa_p$\;
 	}
 }
\For{$p = 2, \cdots , d$}{
	\For{$m = 1, \cdots , p-1$}{
		$\theta_{m,p} = \theta_{m,p-1} - \kappa_p \theta_{p-m,p-1}$\;
	}
}
 \caption{Construction des $\theta$ à partir  des $\kappa$}
 \label{algo:construction}
\end{algorithm}
\begin{Rque}
Dans cet algorithme, à chaque étape on calcule les $(\theta_{1,p}, \cdots, \theta_{p-1,p})$ grâce aux $(\theta_{1,p-1}, \cdots, \theta_{p-1,p-1})$ calculés à l'étape d'avant :
\begin{itemize}
\item $p=2$ : $\theta_{1,2} = \theta_{1,1} - \kappa_2 \theta_{1,1}$
\item $p=3$ : $\theta_{1,3} = \theta_{1,2} - \kappa_3 \theta_{2,2}$ et $\theta_{2,3} = \theta_{2,2} - \kappa_3 \theta_{1,2}$\\
...
\end{itemize} 
\end{Rque}
\begin{Thm}
\label{thm:kappa}
Quels que soient $(\kappa_1,\cdots, \kappa_d)\in \seg{-1,1}^d$, $\forall p \in \iseg{1,d}$, les coefficients $(\theta_{1,p}, \cdots, \theta_{p,p})$ construits par l'algorithme \ref{algo:construction} sont les coefficients d'un processus $AR(p)$ causal i.e 
$$
1-\sum_{k=1}^p \theta_{k,p} z^k \neq 0 \,\, \forall \abs{z} \leq 1
$$
\end{Thm}
\begin{proof}
J'arrive pas...
\end{proof}
\chapter{Processus TVAR}
\section{Construction d'une solution stable}
\paragraph{Equation TVAR :}
\begin{equation} \label{eq:TVAR}
X_t = \sum_{i=1}^p \theta_i(t) X_{t-i} + \sigma(t) \epsilon_t
\tag{TVAR}
\end{equation}
où $\epsilon$ est un bruit blanc centré de variance unitaire.
\paragraph{Condition de stabilité :}
Le critère qui nous intéresse est d'avoir une solution de \eqref{eq:TVAR} vérifiant la condition de stabilité suivante :
\begin{equation} \label{eq:stabilite}
\sup_{t\in \zset} \EE[|X_t|^2] < +\infty
\tag{S}
\end{equation}
\subsection{Cas simple}
On considère ici $\sigma(t)=1 \, \forall t\in \rset$ et $\theta_i(t) = \piecewise{
0 & t<0 \\
\theta_i & t \geq 0 \\ 
} $. 
\begin{Prop}\label{prop:cas_simple_unicite}
Dans ce cas particulier, l'équation \eqref{eq:TVAR} admet une unique solution
\end{Prop} 
\begin{proof}
Tout d'abord pour $t < 0$ on a $X_t = \epsilon_t$ \\
Pour $t \geq 0$, notons $\mb{X}_k = [ X_k, \cdots , X_{k-p+1} ]^T$, $\mb{e}_1 = [1,0, \cdots, 0]^T$ et $A = \begin{pmatrix}
\theta_1 & \theta_2 & \cdots & \cdots & \theta_p \\
1 & 0 & \cdots & \cdots & 0 \\
0 & \ddots & \ddots & & \vdots \\
\vdots & \ddots & \ddots & \ddots & 0 \\
0 & \cdots & 0 & 1 & 0
\end{pmatrix}$. Alors l'équation \eqref{eq:TVAR} s'écrit : 
\[ \mb{X}_t = A\mb{X}_{t-1} + \epsilon_t \mb{e}_1 \]
En itérant pour $t-1, t-2$ etc, on obtient 
\[ \forall k\geq 0 \,  \mb{X}_t = A^{k+1} \mb{X}_{t-k-1} + \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  \]
Considérons $k \geq t$ alors $\mb{X}_{t-k-1} = [ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]^T$. Ainsi 
\begin{equation}\label{eq:X}
\forall k\geq t \,  \mb{X}_t = A^{k+1} \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix} 
+ \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  
\end{equation}
ce qui fournit une définition $X_t = \mb{e}_1^T \mb{X}_t$ unique
\end{proof}

On cherche alors une condition sur les $(\theta_i)_{i=1}^p$ pour cette solution vérifie la condition de stabilité \eqref{eq:stabilite}


\subsubsection{Cas où p=1}
\begin{Prop}
Si $p=1$ on note $\theta(t)= \piecewise{
0 & t<0 \\
\theta & t \geq 0 \\} $ et \eqref{eq:TVAR} devient $X_t = \theta(t)X_{t-1} + \sigma(t) \epsilon_t$. La solution de l'équation vérifie la condition de stabilité \eqref{eq:stabilite} si et seulement si $|\theta| < 1$
\end{Prop}
\begin{proof}
Si $t < 0\, X_t = \epsilon_t$ donc $\sup_{t < 0} \EE[|X_t|^2] = 1$. Si $t\geq 0$, la formule de $X_t$ donnée par \eqref{eq:X} se traduit pour $p=1$ par
$ \forall k \geq t \,  X_t = \theta^{k+1}\epsilon_{t-k-1} + \sum_{j=0}^k \theta^j \epsilon_{t-j} $ i.e $\forall k \geq t \, X_t = \sum_{j=0}^{k+1} \theta^j\epsilon_{t-j}$.
Ainsi 
\[ \forall t \in \nset, \, \forall k \geq t \, \EE[|X_t|^2] = \sum_{i=0}^{k+1} \sum_{j=0}^{k+1} \theta^i \overline{\theta}^j \EE[\epsilon_{t-i} \epsilon_{t-j}]= \sum_{i=0}^{k+1} |\theta|^{2i} \]
Ainsi en faisant tendre $k$ vers $+\infty$ on obtient 
 \[ \forall t \in \nset, \, \EE[|X_t|^2] = \sum_{i=0}^{+\infty} |\theta|^{2i} = \piecewise{
+\infty & |\theta|\geq 1 \\
\frac{1}{1-|\theta|^2} & |\theta| < 1} \]
Ce qui donne $\sup_{t\in \nset} \EE[|X_t|^2] < +\infty \iff |\theta| < 1$ et comme sur $\sup_{t < 0} \EE[|X_t|^2] = 1$ la condition est valable pour le sup sur $t\in \zset$
\end{proof}

\subsubsection{Cas p quelconque}
Pour prouver ce cas, quelques lemmes d'algèbre linéaire sont nécessaires
\begin{Lem}
La matrice $A$ définie dans la preuve de la propriété \ref{prop:cas_simple_unicite} a pour polynôme caractéristique : 
\[
\chi_A (X)= \det(A-XI_p) = (-1)^p \left( X^p - \sum_{i=1}^p \theta_i X^{p-i} \right)
\]
\end{Lem}
\paragraph{Conséquence :}
Les valeurs propres de $A$ sont les inverses des racines de $P = 1- \sum_{i=1}^p \theta_i X^i$
car $\chi_A = (-1)^p X^p P\left( \frac{1}{X}\right)$
\begin{Lem}\label{lem:approx_rayon_spec}
Soit $A \in \cset^{p\times p}$ alors, on rappelle la définition de la norme subordonnée associée à une norme $\norm{.}$ sur $\cset^p$ : \[
\tnorm{A} = \sup_{\norm{x} = 1} \norm{Ax}
\]
On rappelle aussi la définition du rayon spectral $ \rho(A) = \max_{\lambda \in spec(A)} \abs{\lambda}$. On a la propriété suivante : \\
Pour tout $A \in \cset^{p\times p}$, pour tout  $\epsilon >0$ il existe une norme sur $\cset^p$ dépendant de $\epsilon$ et de $A$ telle que la norme subordonnée correspondante $\tnorm{.}_{\epsilon,A}$ vérifie
\[
\tnorm{A}_{\epsilon,A} \leq \rho(A) + \epsilon
\]
\end{Lem}
\begin{Prop}[Condition suffisante]
Dans ce cas particulier, en notant $P(z) = 1 - \sum_{i=1}^p \theta_i z^i$, si $\forall |z| \leq 1\, P(z)\neq 0$ (i.e les racines de $P$ sont hors du disque unité fermé) alors la solution de l'équation \eqref{eq:TVAR} vérifie la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
On part de la définition de $\mb{X}_t$ pour $t \in \nset $ donnée par \eqref{eq:X}

De plus $X_t = \mb{e}_1^T \mb{X}_t$ donc $\EE[|X_t|^2] = \EE[X_t \overline{X}_t^T] = \mb{e}_1^T\EE[\mb{X}_t \mb{X}_t^T] \mb{e}_1$ avec $\forall k \geq t$, comme $\epsilon$ est un bruit blanc
\begin{align*}
\EE[\mb{X}_t \mb{X}_t^T]  
&= \sum_{j=0}^k \sum_{l=0}^k A^j \mb{e}_1 \EE[\epsilon_{t-j} \epsilon_{t-l}] \mb{e}_1^T {(A^l)}^T  + A^{k+1} \EE[ \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix}  {[ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]} ] {(A^{k+1})}^T \\
& = \sum_{j=0}^k A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T + A^{k+1} {(A^{k+1})}^T
\end{align*}
Or on a supposé que les racines de $P$ sont de module strictement supérieur à 1 donc les valeurs propres de $A$ sont de module strictement inférieur à 1. Ainsi $\rho(A) < 1$, il existe donc $\epsilon > 0$ tel que $\rho(A) + \epsilon < 1$. En appliquant le lemme \ref{lem:approx_rayon_spec} à $\epsilon$ et $A$ on obtient (en notant juste $\tnorm{A}$ au lieu de $\tnorm{A}_{A,\epsilon}$ pour alléger les notations) $\tnorm{A} < 1$. \\
Ceci implique dans un premier temps $\tnorm{A^k} \leq \tnorm{A}^k \xrightarrow[k \rightarrow +\infty]{} 0$ donc $A^k \xrightarrow[k \rightarrow +\infty]{} 0$ et ainsi $A^{k+1} (A^{k+1})^T \xrightarrow[k \rightarrow +\infty]{} 0$ \\
De plus $\forall j\in \nset$ $\tnorm{A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T}
\leq \tnorm{A}^{2j} \tnorm{\mb{e}_1 \mb{e}_1^T}$  qui est terme général d'une série convergence dans $\rset$ car $\tnorm{A} < 1$ donc la série des $A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T$ est absolument convergente donc convergente dans $\rset^{p\times p}$. \\
En faisant donc tendre $k$ vers $+\infty$ dans l'expression de $\EE[\mb{X}_t \mb{X_t}^T]$ on obtient : 
\[
\EE[\mb{X}_t \mb{X_t}^T] = \sum_{j=0}^{+\infty} A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T \in \rset^{p\times p}
\]
Ainsi $\EE[|X_t|^2]$ étant le premier coefficient de cette matrice, $\EE[|X_t|^2] < +\infty$
\end{proof}
\subsection{Cas général avec p=1}
L'équation \eqref{eq:TVAR} devient $X_t = \theta(t)X_{t-1} + \sigma(t) \epsilon_t$
\begin{Prop}
Si $\sup_t |\theta(t)| < 1$ et $\sup_t |\sigma(t)| < +\infty$ alors il existe un unique processus $(X_t)_{t \in \zset}$ vérifiant à la fois \eqref{eq:TVAR} et la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
En itérant $k$ fois l'équation on obtient 
\begin{equation}\label{eq:p_1_iter} 
\forall k \geq 0 \, X_t = \left( \prod_{j=0}^{k} \theta(t-j)  \right) X_{t-k-1} + \sum_{j=0}^k \left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)\epsilon_{t-j}
\end{equation}
\begin{itemize}
\item Supposons que $X$ vérifie la condition de stabilité et appelons $M = \sup_t \EE[\abs{X_t}^2]$, $\theta_{\max} = \sup_t |\theta(t)|$ et $\sigma_{\max} = \sup_t |\sigma(t)|$ alors on a $\forall k \geq 0$ :
\[
\norm{\left( \prod_{j=0}^{k} \theta(t-j)  \right) X_{t-k-1} }_2^2 = \left( \prod_{j=0}^{k} \abs{\theta(t-j)}^2 \right) \EE[\abs{X_{t-k-1}}^2] \leq \theta_{\max}^{2(k+1)} M
\]
Ainsi $\lim_{k \rightarrow +\infty} \norm{\left( \prod_{j=0}^{k} \theta(t-j)  \right) X_{t-k-1} }^2 = 0$ car on a pris $\theta_{\max} < 1$.\\ 
De plus $\forall j \geq 0$
\begin{equation}\label{eq:p_1_sommabilite}
\norm{\left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2^2 =  \left( \prod_{i=0}^{j-1} \abs{\theta(t-i)}^2 \right) \abs{\sigma(t-j)}^2 \EE[\abs{\epsilon_{t-j}}^2]
= \left( \prod_{i=0}^{j-1} \abs{\theta(t-i)}^2 \right) \abs{\sigma(t-j)}^2
\leq \theta_{\max}^{2j} \sigma_{\max}^2
\end{equation}
Ce qui donne 
\[
\norm{\left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2 \leq \theta_{\max}^{j} \sigma_{\max}
\]
Encore une fois, parce qu'on a pris $\theta_{\max}<1$, on a à droite de l'inégalité le terme général d'une série convergente. Ainsi en notant $c_j = \left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)$ la série de terme général $c_j \epsilon_{t-j}$ est absolument convergente donc convergente. \\
On peut alors faire tendre $k$ vers $+\infty$ dans \eqref{eq:p_1_iter}, on obtient 
\begin{equation}\label{eq:p_1_solution}
X_t = \sum_{j=0}^{+\infty} c_j  \epsilon_{t-j} \text{ avec } c_j = \left( \prod_{i=0}^{j-1} \theta(t-i) \right) \sigma(t-j)
\end{equation}
\item Définissons maintenant $X$ comme dans l'équation \eqref{eq:p_1_solution} et montrons qu'elle vérifie l'équation TVAR et la condition de stabilité. \\
Tout d'abord pour tout $t\in \zset$ on a 
\begin{align*}
\theta(t) X_{t-1} + \sigma(t) \epsilon_t 
&= \theta(t) \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} \theta(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} +\sigma(t) \epsilon_t \\
&= \sum_{j=-1}^{+\infty} \left( \prod_{i=-1}^{j-1} \theta(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} \\
&\underset{(j \leftarrow j+1)}{=}  \sum_{j=0}^{+\infty} \left( \prod_{i=-1}^{j-2} \theta(t-1-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&\underset{(i \leftarrow i+1)}{=} \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} \theta(t-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&= X_t
\end{align*}
\item Remarquons tout d'abord que \eqref{eq:p_1_sommabilite} prouve que $(c_j)_{j\in \nset} \in \ell^2(\nset)$. On a alors $\forall t\in \zset$ 
\begin{align*}
\EE[\abs{X_t}^2] 
&= \EE[\abs{\lim_{k \rightarrow +\infty} \sum_{j=0}^k c_j \epsilon_{t-j}}^2] \\
&= \lim_{k \rightarrow +\infty} \EE[\abs{\sum_{j=0}^k c_j \epsilon_{t-j}}^2] \text{ (par continuité de l'espérance)}\\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \EE[\abs{\epsilon_{t-j}}^2] \text{ (car } \epsilon \text{ est un bruit blanc) } \\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \\
&= \sum_{k=0}^{+\infty}  \abs{c_j}^2
\end{align*}
Ce résultat étant indépendant de $t$ on a bien 
\[
\sup_t \EE[\abs{X_t}^2] = \sum_{k=0}^{+\infty}  \abs{c_j}^2 < +\infty
\]
\end{itemize}

\end{proof}
\subsection{Contre-exemple avec p=2}

On va montrer que la condition obtenue pour le cas $p=1$ n'est plus suffisante pour $p=2$.

Pour ce contre-exemple, on va considérer le TVAR(2) définit comme ceci :
\begin{align*}
X_{2t} = a X_{2t-1} + \epsilon_{2t} \\
X_{2t+1} = b_1 X{2t} + b_2 X_{2t-1} + \epsilon_{2t+1}
\end{align*}

pour $t>0$ sinon $X_t = \epsilon_t$.

On a alors :
$$
\forall t>0, X_{2t+1} = (ab_1 + b_2) X_{2t-1} + b_1 \epsilon_{2t} + \epsilon_{2t+1}
$$
On a alors que le processus $Y_t = X_{2t+1}$ suit l'équation d'un AR(1) dont la condition de stabilité implique :
\begin{equation} \label{eq:racines}
|ab_1 + b_2| < 1
\tag{*}
\end{equation}

D'autre part, le polynôme caractéristique associé est $P(z) = 1 - b_1 z - b_2 z^2$. Posons $P(z) = (1-bz)^2$. On a alors $b_1 = 2b$ et $b_2 = -b^2$.
La condition \eqref{eq:racines} donne:
$$
|2ba-b^2| <1
$$
Avec $a = -b$, celle-ci devient :
$$
3b^2 < 1
$$
Or ceci peut être faux. Il suffit de prendre $b=\frac{1}{\sqrt{2}}$ par exemple.

On a donc trouvé une sous-suite $Y_t$ du processus $X_t$ qui diverge. Donc la condition initiale, à savoir $\sup_t|\theta_i(t)|<1$, n'est plus suffisante pour assurer la stabilité d'un TVAR(2).

\subsection{Cas général}
On introduit une nouvelle définition du modèle TVAR introduite dans \citep{Dahlhaus:1996} :
\begin{Def} \label{def:TVAR}
Soient $p\geq 1$, $\theta_1,\cdots, \theta_p$ et $\sigma$ des fonctions définies sur $]-\infty,1]$ et $(\epsilon_t)_{t\in \zset}$ une suite i.i.d de variables aléatoires avec moyenne nulle et variance unitaire. Pour tout $T \geq 1$ on dit que $(X_{t,T})_{t\leq T}$ est un processus TVAR stable s'il vérifie les deux conditions suivantes 
\begin{description}
\item[(i)] $\forall -\infty < t \leq T $
\begin{equation}\label{eq:TVAR'}
X_{t,T} = \sum_{i=1}^p \theta_i\left( \frac{t}{T}\right) X_{t-i,T} + \sigma\left( \frac{t}{T} \right) \epsilon_t
\tag{TVAR'}
\end{equation}
\item[(ii)]
\begin{equation}\label{eq:S'}
\sup_{-\infty < t\leq T} \EE[\abs{X_{t,T}}^2] < +\infty
\tag{S'}
\end{equation}
\end{description}
\end{Def}
\begin{Rque}
Revenons sur cette nouvelle définition du modèle TVAR. Supposons $u \in ]-\infty , 1]$ fixé et $T \gg 1$. Si on veut étudier ce qu'il se passe pour $t$ dans un intervalle du type $[(u-b)T,(u+b)T ]$, alors on peut prendre b petit sans avoir un intervalle trop petit pour $t$. Alors $\frac{t}{T}\in [u-b,u+b]$ reste proche de u et si $\theta_i$ et $\sigma$ sont suffisamment régulières, on peut approcher les approcher par des constantes. Ainsi l'équation s'approche par une équation AR.

\end{Rque}
\begin{Prop}\label{prop:TVAR}
Supposons que les coefficients $\theta_i$ de l'équation \eqref{eq:TVAR} sont uniformément continus sur $]-\infty,1]$ et que $\sigma$ est bornée sur $]-\infty,1]$. Supposons de plus qu'il existe $\delta \in ]0,1[$ tel que $\Theta(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]$ où 
\[
\Theta(z;u) = 1 - \sum_{i=1}^p \theta_i(u) z^i
\]
Alors il existe $T_0 \geq 1$ tel que $\forall T \geq T_0$ il existe un unique processus $(X_{t,T})_{t\leq T}$ vérifiant \eqref{eq:TVAR'} et \eqref{eq:S'}. De plus cette solution s'écrit : 
\begin{equation}\label{eq:repr_lineaire}
\forall -\infty < t \leq T
 \,\, X_{t,T} = \sum_{j=0}^{+\infty} \phi_{t,T} (j) \sigma\left( \frac{t-j}{T} \right) \epsilon_{t-j}
\end{equation}
avec $\forall \delta_1 \in ]\delta, 1[$
$$
\sup_{T \geq T_0} \sup_{-\infty < t \leq T} \sup_{j\geq 0} \delta_1^{-1} \abs{\phi_{t,T}(j)} < +\infty
$$

\end{Prop}
\begin{proof}
La preuve de cette proposition peut être trouvée dans \citep{giraud-roueff-sanchez-aos2015}
\end{proof}
\section{Prédiction sur un TVAR}
\paragraph{Détail pratique pour l'implémentation :} Dans cette partie, nous considèrerons que les coefficients sont constantes pour $u \leq 0$ : $\theta_i(u) = \theta_i(0)$ si $u \leq 0$. Dans la pratique nous prendrons $0$. Nous allons donc nous restreindre à $[0,1]$
\subsection{Implémentation d'un TVAR}
\paragraph{Coefficients AR abstraits associés}
Il va être intéressant pour la suite d'adopter la vision suivante : \\
On se donne $p$ fonctions $\theta_1, \cdots, \theta_p$ vérifiant les hypothèses de la proposition \ref{prop:TVAR}. Alors à $T$ donné et $-\infty < t \leq T$ fixé si on note $u = \frac{t}{T}$ les coefficients $(\theta_1(u), \cdots , \theta_p(u))$ sont ceux d'un AR(p) causal car $\Theta(z;u) \neq 0 \,\, \forall |z| \leq 1$.\\
On peut donc à $t$ fixé se ramener à ce que l'on connaît sur les processus AR en considérant un processus AR(p) abstrait associé à ces coefficients (notamment utiliser l'algorithme de Levinson-Durbin on considérer la densité spectrale de puissance). Il y un processus AR(p) abstrait local à chaque temps $t$ et notamment une densité spectrale de puissance locale à chaque instant $t$. \\

Maintenant que nous considérons à chaque instant $u$ un processus AR(p) local nous pouvons appliquer l'algorithme \ref{algo:construction} et le théorème \ref{thm:kappa} pour chaque instant $u$ à partir de $p$ fonctions continues $\kappa_i :[0,1] \to [-1,1]$ pour $i=1,\cdots,p$
\begin{Thm}
\label{thm:kappa_u}
Quelles que soient les fonctions $\kappa_1, \cdots, \kappa_p$ continues sur $[0,1]$ et à valeurs dans $[-1,1]$, les fonctions $\theta_{1,p},\cdots, \theta_{p,p}$ obtenues en appliquant l'algorithme \ref{algo:construction} à chaque instant $u$ sont les coefficients d'un processus TVAR(p) stable i.e elles sont uniformément continues sur $[0,1]$ et il existe $\delta \in ]0,1[$ tel que $\Theta(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]$ où 
\[
\Theta(z;u) = 1 - \sum_{i=1}^p \theta_{i,p}(u) z^i
\]
\end{Thm}
\begin{proof} 
Tout d'abord les $(\theta_{i,p})_{i\in \iseg{1,p}}$ sont obtenus par sommes et produits des $(\kappa_i)_{i\in \iseg{1,p}}$ donc sont continus sur $[0,1]$ donc uniformément continus sur ce segment. \\
Supposons maintenant $\forall \delta \in ]0,1[, \exists \abs{z} < \delta^{-1}, u\in [0,1], \Theta(z;u) = 0$. Considérons une suite $(\delta_n)_{n\in \nset} \in ]0,1[^\nset$ telle que $\delta_n \uparrow 1$. Alors il existe une suite $(z_n)_{n\in \nset}$ et une suite $(u_n)_{n\in \nset}$ telle que $\forall n \in \nset$
$$
\abs{z_n} < \delta_n^{-1},
\,\,
u_n \in [0,1] 
\mbox{ et }
\Theta(z_n;u_n) = 0
$$
On peut alors prendre une sous suite $(z_{\alpha_n}, u_{\alpha_n})_{n\in \nset}$ de $(z_n,u_n)_{n\in \nset}$ qui converge vers un couple $(z,u)$ tel que $\abs{z} \leq 1$ et $u\in [0,1]$. On a en effet $\abs{z} \leq 1$ car $\forall n\in \nset, \abs{z_{\alpha_n}} < \delta_{\alpha_n}^{-1}$ et par passage à la limite $\abs{z} \leq 1$ \\
De plus par continuité de $\Theta$, en passant à la limite dans $\forall n\in \nset, \Theta(z_{\alpha_n}, u_{\alpha_n}) = 0$ on obtient finalement :
$$
\abs{z} \leq 1,
\,\,
u \in [0,1] 
\mbox{ et }
\Theta(z;u) = 0
$$
Ce qui contredit le résultat du théorème \ref{thm:kappa} appliqué à $\kappa_1(u),\cdots,\kappa_p(u)$ qui dit que $\Theta(z;u) \neq 0, \forall \abs{z}\leq 1$
\end{proof}

\subsection{Définition de l'estimateur des coefficients TVAR}
On considère un processus TVAR $X_{t,T}$ définit grâce à la proposition \ref{prop:TVAR}. Notons que l'équation \eqref{eq:TVAR'} se réécrit de manière plus compacte par : $\forall t \in \iseg{1,T}$
$$
X_{t,n} = \bftheta_{t-1,T}^T \mb{X}_{t-1,T} + \sigma_{t,T} \epsilon_{t,T}
$$
où 
\begin{align*}
&\mb{X}_{t,T} = [X_{t,T}, X_{t-1,T}, \cdots , X_{t-p+1,T} ]^T \\
&\bftheta_{t,T} = \bftheta\left(\frac{t}{T}\right) = \left[ \theta_1\left(\frac{t}{T}\right), \theta_2\left(\frac{t}{T}\right), \cdots, \theta_p\left(\frac{t}{T}\right) \right]^T \\
&\sigma_{t,T} = \sigma\left(\frac{t}{T}\right)
\end{align*}
\begin{Def}[Estimateur NLMS de $\bftheta$ :]
On se donne un pas $\mu$, l'estimateur est le suivant :
\begin{equation}\label{eq:NLSM}
\begin{array}{l}
\hat{\bftheta}_{0,T}(\mu) = 0 \\
\forall t \in \iseg{0,T-1}, \, \hat{\bftheta}_{t+1,T}(\mu) = \hat{\bftheta}_{t,T}(\mu) + \mu ( X_{t+1,T} - \hat{\bftheta}_{t+1,T}(\mu)^T \mb{X}_{t,T} ) \frac{\mb{X}_{t,T}}{1+\mu \norm{\mb{X}_{t,T}}^2}
\end{array}
\tag{NLMS}
\end{equation}
On définit ensuite l'estimateur de $\bftheta : u \rightarrow \bftheta(u) = \left[ \theta_1(u), \theta_2(u), \cdots, \theta_p(u) \right]^T$ pour $u\in [0,1]$ en faisant une interpolation :
$$
\hat{\bftheta}_T(u ; \mu) = \hat{\bftheta}_{\floor{uT},T}(\mu)
$$
A un instant $u$, l'estimateur est une fonction de $\mb{X}_{0,T}, X_{1,T}, \cdots , X_{\floor{uT},T}$ et $\mu$
\end{Def}
\begin{Def}[$\beta$-Lipschitz semi-norme]
Soit $\beta \in ]0,1]$ et $\mb{f} : [0,1] \to \rset^d$, on définit la semi-norme $\beta$-Lipschitz de $f$ par :
$$
\abs{\mb{f}}_{\Lambda,\beta} = \sup_{t\neq s} \frac{\norm{\mb{f}(t)-\mb{f}(s)}}{\abs{t-s}^\beta}
$$
On définit la boule $\beta$-Lipschitz de rayon $L > 0$ : 
$$
\Lambda_d(\beta,L) = \ens{ \mb{f} : [0,1] \to \rset^d, \abs{\mb{f}}_{\Lambda,\beta} \leq L, \sup_{t\in [0,1]} \norm{\mb{f}(t)} \leq L }
$$
\end{Def}
\paragraph{Notations :}
On notera pour $Z$ v.a à valeur dans un espace muni d'une norme $\norm{.}$ : $\norm{Z}_q = (\EE[\norm{Z}])^{1/q}$ et  $\norm{Z}_{q,\bftheta,\sigma} = (\EE_{\bftheta,\sigma}[\norm{Z}])^{1/q}$ dans le cas où l'on a une paramétrisation par $(\bftheta,\sigma)$ \\
On note de plus 
\begin{itemize}
\item pour $\rho > 0$ : 
$$
S(\rho) = \ens{\bftheta = [\theta_1, \cdots , \theta_p]^T : [0,1] \to \rset^p, \Theta(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]}
$$
\item pour $\beta \in ]0,1], L >0, \rho \in ]0,1[, 0 < \sigma_- \leq \sigma_+ < +\infty $ : 
$$
C(\beta, L, \rho, \sigma_-, \sigma_+) = \ens{(\bftheta,\sigma), \bftheta \in \Lambda_d(\beta,L) \cap S(\rho), \sigma : [0,1] \to [\sigma_-, \sigma_+]}
$$
\end{itemize}

\begin{Thm}
On suppose que $\epsilon$ est un bruit blanc centré de variance unitaire indépendant de $\mb{X}_{0,T}$ et de plus que $\sup_{T \geq 1} \norm{\mb{X}_{0,T}}_q < +\infty$ et $\sup_{1\leq t \leq T} \norm{\epsilon_{t,T}}_q < +\infty$ avec $q \geq 4$. \\
Soient $p\in [1,q/3[, \beta\in ]0,1], L>0, \rho \in ]0,1[$ et $0 < \sigma_{-} \leq \sigma_{+}$ \\
Alors $\exists M,\delta > 0$ et $\mu_0 >0$ tels que $\forall \mu \in ]0,\mu_0], T \geq 1, u\in ]0,1]$ et $(\bftheta, \sigma) \in C(\beta, L, \rho, \sigma_-, \sigma_+)$ 
$$
\norm{\hat{\bftheta}_T(u ; \mu) - \bftheta(u)}_{p,\bftheta,\sigma} \leq M \left( \norm{\bftheta(0)}(1-\delta \mu)^{uT} + \sqrt{\mu} + (T\mu)^{-\beta} \right)
$$
\end{Thm}
\begin{proof}
Le preuve peut être trouvée dans \citep{moulines-priouret-roueff-2005}
\end{proof}
\begin{Rque}
Cette borne supérieure peut s'interpréter de la manière suivante :
\begin{itemize}
\item Le premier terme $\norm{\bftheta(0)}(1-\delta \mu)^{uT}$ traduit l'oubli de l'erreur initiale de l'estimateur quand $T$ augmente
\item Le deuxième terme $\sqrt{\mu}$ est un terme de variance de l'estimateur qui serait là même si $u \mapsto \bftheta(u)$ était constante
\item Le dernier terme $(T\mu)^{-\beta}$ contrôle l'erreur due à l'évolution de $\bftheta$ dans le temps. Elle décroit d'autant plus vite que $\beta$ est grand, ce qui implique une bonne régularité de $\bftheta$
\end{itemize}
\end{Rque}
\subsection{Analyse en densité spectrale de puissance}
Comme expliqué précédemment, on considère pour un processus TVAR donné, les processus AR abstraits à chaque instant $u\in [0,1]$, on définit ainsi les DSP "locales" comme les DSP associées à chaque AR.
\begin{Def}[DSP]
La DSP locale théorique en $u\in [0,1]$ est définie pour $\lambda \in [-\pi, \pi[$ 
$$
S_x(\lambda ; u) = \frac{\sigma^2(u)}{2\pi\abs{\Theta\left( e^{-i\lambda} ; u \right)}^2}
$$
où $\Theta(z;u) = 1 - \sum_{k=1}^p \theta_k(u) z^k$ \\
La DSP locale estimée est : 
$$
\hat{S}_x(\lambda ; u) = \frac{\hat{\sigma}^2(u)}{2\pi\abs{\hat{\Theta}\left( e^{-i\lambda} ; u \right)}^2}
$$
où $\hat{\Theta}(z;u) = 1 - \sum_{k=1}^p \hat{\theta}_k(u) z^k$
\end{Def}
\begin{Prop}
$\forall u \in [0,1]$, considérons $z_1(u),\cdots, z_p(u)$ les inverses des racines de $\Theta(z;u)$ alors en notant $\forall n\in \iseg{1,p}, z_n(u) = \rho_n(u) e^{i \phi_n(u)}$ avec $0 < \rho_n(u) <1$ et $\phi_n(u) \in [-\pi, \pi[$ on a 
$$
S_x(\lambda;u) \mbox{ admet un pic en $\lambda$ } \iff \exists n\in \iseg{1,p} \, \lambda = \phi_n(u)
$$
\end{Prop}
\begin{proof}
Soit $u\in [0,1]$ alors écrivons $\Theta(z;u) = \prod_{n=1}^p (1 - z_n(u)z) = $ alors
$$
\abs{\Theta\left( e^{-i\lambda} ; u \right)}^2 = \prod_{n=1}^p \abs{1 - \rho_n(u) e^{i(\phi_n(u)-\lambda)}}^2
$$
Or $\forall n \in \iseg{1,p}$
$$
\abs{1 - \rho_n(u) e^{i(\phi_n(u)-\lambda)}}^2 = 1 - 2\rho_n(u) \cos(\phi_n(u)-\lambda) + \rho_n^2(u)
$$
est minimal pour $\lambda = \phi_n(u) \mod 2\pi$ \\
MAIS ceci ne veut pas forcément dire que le produit est minimal ssi $\exists n\in \iseg{1,p}, \lambda = \phi_n(u) \mod 2\pi$. Prenons par exemple $2$ inverses de racines conjuguées avec $\rho_1 = \rho_2 = 1/2$ et $\phi_1 = -\phi_2  = \frac{\pi}{4}$. Alors
$$
\abs{\Theta\left( e^{-i\lambda} ; u \right)}^2
= \left( \frac{5}{4} - \cos\left(\lambda - \frac{\pi}{4} \right) \right) \left( \frac{5}{4} - \cos\left(\lambda + \frac{\pi}{4} \right) \right)
$$
Pour $\lambda = 0$ on obtient : $\frac{(5-2\sqrt{2})^2}{16}$ \\
Pour $\lambda = \pm \frac{\pi}{4}$ on obtient : $\frac{5}{16}$ \\
Or $\frac{(5-2\sqrt{2})^2}{16} - \frac{5}{16} = \frac{28 - 20\sqrt{2}}{16} < 0$ donc le minimum n'est pas atteint en $\frac{\pi}{4}$
\end{proof}

\chapter{Implémentations}
\section{Etude spectrale sur un TVAR(1) et un TVAR(2) à partir des racines}
\subsection{Cas d'un TVAR(1)}
\renewcommand\listingscaption{Code}
\begin{listing}
\caption{TVAR(1) à $\phi$ fixé}
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=bg,
fontsize=\footnotesize,
%linenos
]{python}
T = 1000
X = np.zeros(2*T, dtype='complex')
a = np.zeros(2*T, dtype='complex')

u = np.arange(T, dtype='double')/T

rho = 1-np.exp(-u)
phase = 0.5
phi = 2.*np.pi*phase

a[T:2*T] = rho*np.exp(1j*phi)

X[0] = epsilon[0]
    
for n in np.arange(2*T-1):
    X[n+1] = a[n+1]*X[n-1] + epsilon[n+2]
    
t = np.arange(2*T) - T #horizon des temps

plt.plot(t[T:2*T], np.real(X[T:2*T]))
plt.title("Processus pour T = 1000")
\end{minted}
\end{listing}

\renewcommand\listoflistingscaption{Table des codes sources}
\listoflistings
\bibliographystyle{plain}
\bibliography{biblio}

\end{document} 
