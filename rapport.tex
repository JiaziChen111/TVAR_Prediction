\documentclass[a4paper,french]{article}

\usepackage{defs}
\usepackage{init}
\title{SIGMA205 : processus TVAR}
\date{}
\begin{document}
\maketitle
\section{Révisions processus AR}
\paragraph{Equation AR(p) :} 
\begin{equation}\label{eq:AR}
X_t = \sum_{i=1}^p a_i X_{t-i} + \epsilon_t
\tag{AR}
\end{equation}
où $\epsilon$ est un bruit blanc centré de variance unitaire.
\subsection{Construction d'une solution stationnaire au second ordre}
\begin{Thm}
Soit $P(z)=1-\sum_{k=1}^p a_k z^k$, supposons que $\forall |z|=1, P(z)\neq 0$ \\
Alors il existe une solution stationnaire au second ordre de \eqref{eq:AR} 
\end{Thm}
\begin{proof}
Pour prouver cela on a besoin du lemme suivant :
\begin{Lem}
Pour $\alpha \in \ell^1$ et $X$ un processus tel que $\sup_t \EE[|X_t|] < +\infty$ on appelle filtrage de $X$ le processus : 
\[ F_\alpha(X) = \left( \sum_{k\in \zset} \alpha_k X_{t-k} \right)_{t\in \zset} \]
Alors si $\alpha,\beta \in \ell^1$ on a $F_\alpha (F_\beta (X)) = X$ si $\alpha \star \beta = \delta$ \\
De plus $\alpha \star \beta = \delta \iff \forall |z|=1 \, \left( \sum_{z\in \zset} \alpha_k z^k \right) \left( \sum_{z\in \zset} \beta_k z^k \right) =1 $ \\
Enfin si $X$ est stationnaire au second ordre $F(X)$ l'est aussi.
\end{Lem}
\begin{Rque}
$F_\alpha = \sum_{k\in \zset} \alpha_i B^k$ où $B$ est l'opérateur de shift $ B : (x_t)_{t\in \zset} \mapsto (x_{t-1})_{t\in \zset}$
\end{Rque}

Le polynôme $P$ peut s'écrire : $P(z)=\prod_{k=1}^p (1-u_k z)$ où les $u_k$ sont les inverses des racines de $P$. L'équation \eqref{eq:AR} s'écrit $P(B)(X)=\epsilon$. Or avec la factorisation obtenue on a : 
\[
P(B) = (1-u_1 B) \circ \cdots \circ (1-u_p B) = F_{\alpha^{(1)}} \circ \cdots \circ F_{\alpha^{(p)}}
\]
où $\alpha_k^{(l)} = \piecewise{
1 & k=0 \\
-u_l & k=1 \\
0 & \text{sinon} \\}
$ \\
Le but désormais est de chercher pour tout $l \in \llbracket 1, p \rrbracket$ un $\beta^{(l)}$ tel que $\alpha^{(l)} \star \beta^{(l)} = \delta$ pour pouvoir inverser la relation. On sait d'après la remarque qu'il suffit de trouver $\beta^{(l)}$ tel que $\frac{1}{1-u_l z} = \sum_{k \in \zset} \beta_k z^k$ pour tout $|z|=1$ \\
On sait de plus que $\forall l \in \llbracket 1, p \rrbracket$ $|u_l| \neq 1$ par hypothèse sur les racines de $P$.\\
Prenons $l\in \llbracket 1,p \rrbracket$ alors deux cas sont possibles :
\begin{itemize}
\item si $|u_l|< 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} = \sum_{k \geq 0} u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
u_l^k & k \geq 0 \\
0 & \text{sinon} \\
}$
\item si $|u_l|> 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} \frac{-u_l^{-1} z^{-1}}{1-u_l^{-1} z^{-1}} = \sum_{k \leq -1} -u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
-u_l^k & k \leq -1 \\
0 & \text{sinon} \\
}$
\end{itemize}
Finalement prenons $F = F_{\beta^{(p)}} \circ \cdots \circ F_{\beta^{(1)}}$, on prends alors 
\[
X = F(\epsilon) 
\]
\end{proof}

\section{Prédiction}
\begin{Prop}
On se donne $X$ stationnaire au second ordre vérifiant \eqref{eq:AR}. On suppose de plus que $X$ est causal i.e $P(z) \neq 0$ pour tout $|z| \leq 1$
On note $\mathcal{H}_t^X = \overline{\mathrm{Vect}\ens{X_s, s\leq t}}$. Alors
\[ \hat{X}_{t+1} = proj(X_{t+1} | \mathcal{H}_t^X ) = \sum_{k=1}^p a_k X_{t+1-k} \]
\end{Prop}
\begin{proof}
$X_{t+1} = \sum_{k=1}^p a_k X_{t+1-k} +\epsilon_{t+1}$ de plus $\forall k \in \llbracket 1,p \rrbracket \, X_{t+1-k} \in \mathcal{H}_t^X$ donc
\[ \hat{X}_{t+1} = \sum_{k=1}^p a_k X_{t+1-k} + proj(\epsilon_{t+1} | \mathcal{H}_t^X ) \]
Or $X$ est causal donc dans la construction de $X$ (cf preuve d'avant) on a $P(z)= \prod_{l=1}^p (1-u_l z)$ où $\forall l |u_l|<1$ ainsi les $\beta^{(l)}$ correspondants sont tous à support dans $\nset$ ce qui entraine que $\psi = \beta^{(p)} \star \cdots \star \beta^{(1)}$ est aussi à support dans $\nset$ et donc 
\[ X_t = \sum_{k=0}^{+\infty} \psi_k \epsilon_{t-k}\] On en déduit que $\mathcal{H}_t^X = \mathcal{H}_t^\epsilon$ et donc comme $\epsilon$ est un bruit blanc $proj(\epsilon_{t+1} | \mathcal{H}_t^X ) = proj(\epsilon_{t+1} | \mathcal{H}_t^\epsilon ) = 0$ d'où la solution
\end{proof}

\section{TVAR}
\paragraph{Equation TVAR :}
\begin{equation} \label{eq:TVAR}
X_t = \sum_{i=1}^p a_i(t) X_{t-i} + \sigma(t) \epsilon_t
\tag{TVAR}
\end{equation}
\paragraph{Condition de stabilité :}
Le critère qui nous intéresse est d'avoir une solution de \eqref{eq:TVAR} vérifiant la condition de stabilité suivante :
\begin{equation} \label{eq:stabilite}
\sup_{t\in \zset} \EE[|X_t|^2] < +\infty
\tag{S}
\end{equation}
\subsection{Cas simple}
On considère ici $\sigma(t)=1 \, \forall t\in \rset$ et $a_i(t) = \piecewise{
0 & t<0 \\
a_i & t \geq 0 \\ 
} $. 
\begin{Prop}\label{prop:cas_simple_unicite}
Dans ce cas particulier, l'équation \eqref{eq:TVAR} admet une unique solution
\end{Prop} 
\begin{proof}
Tout d'abord pour $t < 0$ on a $X_t = \epsilon_t$ \\
Pour $t \geq 0$, notons $\mb{X}_k = [ X_k, \cdots , X_{k-p+1} ]^T$, $\mb{e}_1 = [1,0, \cdots, 0]^T$ et $A = \begin{pmatrix}
a_1 & a_2 & \cdots & \cdots & a_p \\
1 & 0 & \cdots & \cdots & 0 \\
0 & \ddots & \ddots & & \vdots \\
\vdots & \ddots & \ddots & \ddots & 0 \\
0 & \cdots & 0 & 1 & 0
\end{pmatrix}$. Alors l'équation \eqref{eq:TVAR} s'écrit : 
\[ \mb{X}_t = A\mb{X}_{t-1} + \epsilon_t \mb{e}_1 \]
En itérant pour $t-1, t-2$ etc, on obtient 
\[ \forall k\geq 0 \,  \mb{X}_t = A^{k+1} \mb{X}_{t-k-1} + \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  \]
Considérons $k \geq t$ alors $\mb{X}_{t-k-1} = [ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]^T$. Ainsi 
\begin{equation}\label{eq:X}
\forall k\geq t \,  \mb{X}_t = A^{k+1} \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix} 
+ \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  
\end{equation}
ce qui fournit une définition $X_t = \mb{e}_1^T \mb{X}_t$ unique
\end{proof}

On cherche alors une condition sur les $(a_i)_{i=1}^p$ pour cette solution vérifie la condition de stabilité \eqref{eq:stabilite}


\subsubsection{Cas où p=1}
\begin{Prop}
Si $p=1$ on note $a(t)= \piecewise{
0 & t<0 \\
a & t \geq 0 \\} $ et \eqref{eq:TVAR} devient $X_t = a(t)X_{t-1} + \sigma(t) \epsilon_t$. La solution de l'équation vérifie la condition de stabilité \eqref{eq:stabilite} si et seulement si $|a| < 1$
\end{Prop}
\begin{proof}
Si $t < 0\, X_t = \epsilon_t$ donc $\sup_{t < 0} \EE[|X_t|^2] = 1$. Si $t\geq 0$, la formule de $X_t$ donnée par \eqref{eq:X} se traduit pour $p=1$ par
$ \forall k \geq t \,  X_t = a^{k+1}\epsilon_{t-k-1} + \sum_{j=0}^k a^j \epsilon_{t-j} $ i.e $\forall k \geq t \, X_t = \sum_{j=0}^{k+1} a^j\epsilon_{t-j}$.
Ainsi 
\[ \forall t \in \nset, \, \forall k \geq t \, \EE[|X_t|^2] = \sum_{i=0}^{k+1} \sum_{j=0}^{k+1} a^i \overline{a}^j \EE[\epsilon_{t-i} \epsilon_{t-j}]= \sum_{i=0}^{k+1} |a|^{2i} \]
Ainsi en faisant tendre $k$ vers $+\infty$ on obtient 
 \[ \forall t \in \nset, \, \EE[|X_t|^2] = \sum_{i=0}^{+\infty} |a|^{2i} = \piecewise{
+\infty & |a|\geq 1 \\
\frac{1}{1-|a|^2} & |a| < 1} \]
Ce qui donne $\sup_{t\in \nset} \EE[|X_t|^2] < +\infty \iff |a| < 1$ et comme sur $\sup_{t < 0} \EE[|X_t|^2] = 1$ la condition est valable pour le sup sur $t\in \zset$
\end{proof}

\subsubsection{Cas p quelconque}
Pour prouver ce cas, quelques lemmes d'algèbre linéaire sont nécessaires
\begin{Lem}
La matrice $A$ définie dans la preuve de la propriété \ref{prop:cas_simple_unicite} a pour polynôme caractéristique : 
\[
\chi_A (X)= \det(A-XI_p) = (-1)^p \left( X^p - \sum_{i=1}^p a_i X^{p-i} \right)
\]
\end{Lem}
\paragraph{Conséquence :}
Les valeurs propres de $A$ sont les inverses des racines de $P = 1- \sum_{i=1}^p a_i X^i$
car $\chi_A = (-1)^p X^p P\left( \frac{1}{X}\right)$
\begin{Lem}\label{lem:approx_rayon_spec}
Soit $A \in \cset^{p\times p}$ alors, on rappelle la définition de la norme subordonnée associée à une norme $\norm{.}$ sur $\cset^p$ : \[
\tnorm{A} = \sup_{\norm{x} = 1} \norm{Ax}
\]
On rappelle aussi la définition du rayon spectral $ \rho(A) = \max_{\lambda \in spec(A)} \abs{\lambda}$. On a la propriété suivante : \\
Pour tout $A \in \cset^{p\times p}$, pour tout  $\epsilon >0$ il existe une norme sur $\cset^p$ dépendant de $\epsilon$ et de $A$ telle que la norme subordonnée correspondante $\tnorm{.}_{\epsilon,A}$ vérifie
\[
\tnorm{A}_{\epsilon,A} \leq \rho(A) + \epsilon
\]
\end{Lem}
\begin{Prop}[Condition suffisante]
Dans ce cas particulier, en notant $P(z) = 1 - \sum_{i=1}^p a_i z^i$, si $\forall |z| \leq 1\, P(z)\neq 0$ (i.e les racines de $P$ sont hors du disque unité fermé) alors la solution de l'équation \eqref{eq:TVAR} vérifie la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
On part de la définition de $\mb{X}_t$ pour $t \in \nset $ donnée par \eqref{eq:X}

De plus $X_t = \mb{e}_1^T \mb{X}_t$ donc $\EE[|X_t|^2] = \EE[X_t \overline{X}_t^T] = \mb{e}_1^T\EE[\mb{X}_t \mb{X}_t^T] \mb{e}_1$ avec $\forall k \geq t$, comme $\epsilon$ est un bruit blanc
\begin{align*}
\EE[\mb{X}_t \mb{X}_t^T]  
&= \sum_{j=0}^k \sum_{l=0}^k A^j \mb{e}_1 \EE[\epsilon_{t-j} \epsilon_{t-l}] \mb{e}_1^T {(A^l)}^T  + A^{k+1} \EE[ \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix}  {[ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]} ] {(A^{k+1})}^T \\
& = \sum_{j=0}^k A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T + A^{k+1} {(A^{k+1})}^T
\end{align*}
Or on a supposé que les racines de $P$ sont de module strictement supérieur à 1 donc les valeurs propres de $A$ sont de module strictement inférieur à 1. Ainsi $\rho(A) < 1$, il existe donc $\epsilon > 0$ tel que $\rho(A) + \epsilon < 1$. En appliquant le lemme \ref{lem:approx_rayon_spec} à $\epsilon$ et $A$ on obtient (en notant juste $\tnorm{A}$ au lieu de $\tnorm{A}_{A,\epsilon}$ pour alléger les notations) $\tnorm{A} < 1$. \\
Ceci implique dans un premier temps $\tnorm{A^k} \leq \tnorm{A}^k \xrightarrow[k \rightarrow +\infty]{} 0$ donc $A^k \xrightarrow[k \rightarrow +\infty]{} 0$ et ainsi $A^{k+1} (A^{k+1})^T \xrightarrow[k \rightarrow +\infty]{} 0$ \\
De plus $\forall j\in \nset$ $\tnorm{A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T}
\leq \tnorm{A}^{2j} \tnorm{\mb{e}_1 \mb{e}_1^T}$  qui est terme général d'une série convergence dans $\rset$ car $\tnorm{A} < 1$ donc la série des $A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T$ est absolument convergente donc convergente dans $\rset^{p\times p}$. \\
En faisant donc tendre $k$ vers $+\infty$ dans l'expression de $\EE[\mb{X}_t \mb{X_t}^T]$ on obtient : 
\[
\EE[\mb{X}_t \mb{X_t}^T] = \sum_{j=0}^{+\infty} A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T \in \rset^{p\times p}
\]
Ainsi $\EE[|X_t|^2]$ étant le premier coefficient de cette matrice, $\EE[|X_t|^2] < +\infty$
\end{proof}
\subsection{Cas général avec p=1}
L'équation \eqref{eq:TVAR} devient $X_t = a(t)X_{t-1} + \sigma(t) \epsilon_t$
\begin{Prop}
Si $\sup_t |a(t)| < 1$ et $\sup_t |\sigma(t)| < +\infty$ alors il existe un unique processus $(X_t)_{t \in \zset}$ vérifiant à la fois \eqref{eq:TVAR} et la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
En itérant $k$ fois l'équation on obtient 
\begin{equation}\label{eq:p_1_iter} 
\forall k \geq 0 \, X_t = \left( \prod_{j=0}^{k} a(t-j)  \right) X_{t-k-1} + \sum_{j=0}^k \left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)\epsilon_{t-j}
\end{equation}
\begin{itemize}
\item Supposons que $X$ vérifie la condition de stabilité et appelons $M = \sup_t \EE[\abs{X_t}^2]$, $a_{\max} = \sup_t |a(t)|$ et $\sigma_{\max} = \sup_t |\sigma(t)|$ alors on a $\forall k \geq 0$ :
\[
\norm{\left( \prod_{j=0}^{k} a(t-j)  \right) X_{t-k-1} }_2^2 = \left( \prod_{j=0}^{k} \abs{a(t-j)}^2 \right) \EE[\abs{X_{t-k-1}}^2] \leq a_{\max}^{2(k+1)} M
\]
Ainsi $\lim_{k \rightarrow +\infty} \norm{\left( \prod_{j=0}^{k} a(t-j)  \right) X_{t-k-1} }^2 = 0$ car on a pris $a_{\max} < 1$.\\ 
De plus $\forall j \geq 0$
\begin{equation}\label{eq:p_1_sommabilite}
\norm{\left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2^2 =  \left( \prod_{i=0}^{j-1} \abs{a(t-i)}^2 \right) \abs{\sigma(t-j)}^2 \EE[\abs{\epsilon_{t-j}}^2]
= \left( \prod_{i=0}^{j-1} \abs{a(t-i)}^2 \right) \abs{\sigma(t-j)}^2
\leq a_{\max}^{2j} \sigma_{\max}^2
\end{equation}
Ce qui donne 
\[
\norm{\left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2 \leq a_{\max}^{j} \sigma_{\max}
\]
Encore une fois, parce qu'on a pris $a_{\max}<1$, on a à droite de l'inégalité le terme général d'une série convergente. Ainsi en notant $c_j = \left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)$ la série de terme général $c_j \epsilon_{t-j}$ est absolument convergente donc convergente. \\
On peut alors faire tendre $k$ vers $+\infty$ dans \eqref{eq:p_1_iter}, on obtient 
\begin{equation}\label{eq:p_1_solution}
X_t = \sum_{j=0}^{+\infty} c_j  \epsilon_{t-j} \text{ avec } c_j = \left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)
\end{equation}
\item Définissons maintenant $X$ comme dans l'équation \eqref{eq:p_1_solution} et montrons qu'elle vérifie l'équation TVAR et la condition de stabilité. \\
Tout d'abord pour tout $t\in \zset$ on a 
\begin{align*}
a(t) X_{t-1} + \sigma(t) \epsilon_t 
&= a(t) \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} a(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} +\sigma(t) \epsilon_t \\
&= \sum_{j=-1}^{+\infty} \left( \prod_{i=-1}^{j-1} a(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} \\
&\underset{(j \leftarrow j+1)}{=}  \sum_{j=0}^{+\infty} \left( \prod_{i=-1}^{j-2} a(t-1-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&\underset{(i \leftarrow i+1)}{=} \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} a(t-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&= X_t
\end{align*}
\item Remarquons tout d'abord que \eqref{eq:p_1_sommabilite} prouve que $(c_j)_{j\in \nset} \in \ell^2(\nset)$. On a alors $\forall t\in \zset$ 
\begin{align*}
\EE[\abs{X_t}^2] 
&= \EE[\abs{\lim_{k \rightarrow +\infty} \sum_{j=0}^k c_j \epsilon_{t-j}}^2] \\
&= \lim_{k \rightarrow +\infty} \EE[\abs{\sum_{j=0}^k c_j \epsilon_{t-j}}^2] \text{ (par continuité de l'espérance)}\\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \EE[\abs{\epsilon_{t-j}}^2] \text{ (car } \epsilon \text{ est un bruit blanc) } \\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \\
&= \sum_{k=0}^{+\infty}  \abs{c_j}^2
\end{align*}
Ce résultat étant indépendant de $t$ on a bien 
\[
\sup_t \EE[\abs{X_t}^2] = \sum_{k=0}^{+\infty}  \abs{c_j}^2 < +\infty
\]
\end{itemize}

\end{proof}
\subsection{Contre-exemple avec p=2}

On va montrer que la condition obtenue pour le cas $p=1$ n'est plus suffisante pour $p=2$.

Pour ce contre-exemple, on va considérer le TVAR(2) définit comme ceci :
\begin{align*}
X_{2t} = a X_{2t-1} + \epsilon_{2t} \\
X_{2t+1} = b_1 X{2t} + b_2 X_{2t-1} + \epsilon_{2t+1}
\end{align*}

pour $t>0$ sinon $X_t = \epsilon_t$.

On a alors :
$$
\forall t>0, X_{2t+1} = (ab_1 + b_2) X_{2t-1} + b_1 \epsilon_{2t} + \epsilon_{2t+1}
$$
On a alors que le processus $Y_t = X_{2t+1}$ suit l'équation d'un AR(1) dont la condition de stabilité implique :
\begin{equation} \label{eq:racines}
|ab_1 + b_2| < 1
\tag{*}
\end{equation}

D'autre part, le polynôme caractéristique associé est $P(z) = 1 - b_1 z - b_2 z^2$. Posons $P(z) = (1-bz)^2$. On a alors $b_1 = 2b$ et $b_2 = -b^2$.
La condition \eqref{eq:racines} donne:
$$
|2ba-b^2| <1
$$
Avec $a = -b$, celle-ci devient :
$$
3b^2 < 1
$$
Or ceci peut être faux. Il suffit de prendre $b=\frac{1}{\sqrt{2}}$ par exemple.

On a donc trouvé une sous-suite $Y_t$ du processus $X_t$ qui diverge. Donc la condition initiale, à savoir $\sup_t|a_i(t)|<1$, n'est plus suffisante pour assurer la stabilité d'un TVAR(2).

\subsection{Cas général}
On introduit une nouvelle définition du modèle TVAR :
\begin{Def} \label{def:TVAR}
Soient $p\geq 1$, $a_1,\cdots, a_p$ et $\sigma$ des fonctions définies sur $]-\infty,1]$ et $(\epsilon_t)_{t\in \zset}$ une suite i.i.d de variables aléatoires avec moyenne nulle et variance unitaire. Pour tout $T \geq 1$ on dit que $(X_{t,T})_{t\leq T}$ est un processus TVAR s'il vérifie les deux conditions suivantes 
\begin{description}
\item[(i)] $\forall -\infty < t \leq T $
\begin{equation}\label{eq:TVAR'}
X_{t,T} = \sum_{i=1}^p a_i\left( \frac{t}{T}\right) X_{t-i,T} + \sigma\left( \frac{t}{T} \right) \epsilon_t
\tag{TVAR'}
\end{equation}
\item[(ii)]
\begin{equation}\label{eq:S'}
\sup_{-\infty < t\leq T} \EE[\abs{X_{t,T}}^2] < +\infty
\tag{S'}
\end{equation}
\end{description}
\end{Def}
\begin{Prop}\label{prop:TVAR}
Supposons que les coefficients $a_i$ de l'équation \eqref{eq:TVAR} sont uniformément continus sur $]-\infty,1]$ et que $\sigma$ est bornée sur $]-\infty,1]$. Supposons de plus qu'il existe $\delta \in ]0,1[$ tel que $A(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]$ où 
\[
A(z;u) = 1 - \sum_{i=1}^p a_i(u) z^i
\]
Alors il existe $T_0 \geq 1$ tel que $\forall T \geq T_0$ il existe un unique processus $(X_{t,T})_{t\leq T}$ vérifiant \eqref{eq:TVAR'} et \eqref{eq:S'}. De plus cette solution s'écrit : 
\begin{equation}\label{eq:repr_lineaire}
\forall -\infty < t \leq T
 \,\, X_{t,T} = \sum_{j=0}^{+\infty} \phi_{t,T} (j) \sigma\left( \frac{t-j}{T} \right) \epsilon_{t-j}
\end{equation}
avec $\forall \delta_1 \in ]\delta, 1[$
$$
\sup_{T \geq T_0} \sup_{-\infty < t \leq T} \sup_{j\geq 0} \delta_1^{-1} \abs{\phi_{t,T}(j)} < +\infty
$$

\end{Prop}

\section{Prédiction sur un TVAR}
\subsection{Implémentation d'un TVAR}

\subsection{Définition de l'estimateur des coefficients TVAR}
On considère un processus TVAR $X_{t,T}$ définit grâce à la proposition \ref{prop:TVAR}. Notons que l'équation \eqref{eq:TVAR'} se réécrit de manière plus compacte par : $\forall t \in \iseg{1,T}$
$$
X_{t,n} = \bftheta_{t-1,T}^T \mb{X}_{t-1,T} + \sigma_{t,T} \epsilon_{t,T}
$$
où 
\begin{align*}
&\mb{X}_{t,T} = [X_{t,T}, X_{t-1,T}, \cdots , X_{t-p+1,T} ]^T \\
&\bftheta_{t,T} = \bftheta\left(\frac{t}{T}\right) = \left[ a_1\left(\frac{t}{T}\right), a_2\left(\frac{t}{T}\right), \cdots, a_p\left(\frac{t}{T}\right) \right]^T \\
&\sigma_{t,T} = \sigma\left(\frac{t}{T}\right)
\end{align*}
\paragraph{Estimateur NLMS de $\bftheta$ :}
On se donne un pas $\mu$, l'estimateur est le suivant :
\begin{equation}\label{eq:NLSM}
\begin{array}{l}
\hat{\bftheta}_{0,T}(\mu) = 0 \\
\forall t \in \iseg{0,T-1} \, \hat{\bftheta}_{t+1,T}(\mu) = \hat{\bftheta}_{t,T}(\mu) + \mu ( X_{t+1,T} - \hat{\bftheta}_{t+1,T}(\mu)^T \mb{X}_{t,T} ) \frac{\mb{X}_{t,T}}{1+\mu \norm{\mb{X}_{t,T}}^2}
\end{array}
\tag{NLMS}
\end{equation}
On définit ensuite l'estimateur de $\bftheta : u \rightarrow \bftheta(u) = \left[ a_1(u), a_2(u), \cdots, a_p(u) \right]^T$ pour $u\in [0,1]$ en faisant une interpolation :
$$
\hat{\bftheta}_T(u ; \mu) = \hat{\bftheta}_{\floor{uT},T}(\mu)
$$
A un instant $u$, l'estimateur est une fonction de $\mb{X}_{0,T}, X_{1,T}, \cdots , X_{\floor{uT},T}$ et $\mu$

\begin{Def}[$\beta$-Lipschitz semi-norme]
Soit $\beta \in ]0,1]$ et $\mb{f} : [0,1] \to \rset^d$, on définit la semi-norme $\beta$-Lipschitz de $f$ par :
$$
\abs{\mb{f}}_{\Lambda,\beta} = \sup_{t\neq s} \frac{\norm{\mb{f}(t)-\mb{f}(s)}}{\abs{t-s}^\beta}
$$
On définit la boule $\beta$-Lipschitz de rayon $L > 0$ : 
$$
\Lambda_d(\beta,L) = \ens{ \mb{f} : [0,1] \to \rset^d, \abs{\mb{f}}_{\Lambda,\beta} \leq L, \sup_{t\in [0,1]} \norm{\mb{f}(t)} \leq L }
$$
\end{Def}
\paragraph{Notations :}
On notera pour $Z$ v.a à valeur dans un espace muni d'une norme $\norm{.}$ : $\norm{Z}_q = (\EE[\norm{Z}])^{1/q}$ et  $\norm{Z}_{q,\bftheta,\sigma} = (\EE_{\bftheta,\sigma}[\norm{Z}])^{1/q}$ dans le cas où l'on a une paramétrisation par $(\bftheta,\sigma)$ \\
On note de plus 
\begin{itemize}
\item pour $\rho > 0$ : 
$$
S(\rho) = \ens{\bftheta = [a_1, \cdots , a_p]^T : [0,1] \to \rset^p, A(z;u)\neq 0 \, \forall \abs{z} < \delta^{-1}, u\in [0,1]}
$$
\item pour $\beta \in ]0,1], L >0, \rho \in ]0,1[, 0 < \sigma_- \leq \sigma_+ < +\infty $ : 
$$
C(\beta, L, \rho, \sigma_-, \sigma_+) = \ens{(\bftheta,\sigma), \bftheta \in \Lambda_d(\beta,L) \cap S(\rho), \sigma : [0,1] \to [\sigma_-, \sigma_+]}
$$
\end{itemize}

\begin{Thm}
On suppose que $\epsilon$ est un bruit blanc centré de variance unitaire indépendant de $\mb{X}_{0,T}$ et de plus que $\sup_{T \geq 1} \norm{\mb{X}_{0,T}}_q < +\infty$ et $\sup_{1\leq t \leq T} \norm{\epsilon_{t,T}}_q < +\infty$ avec $q \geq 4$. \\
Soient $p\in [1,q/3[, \beta\in ]0,1], L>0, \rho \in ]0,1[$ et $0 < \sigma_{-} \leq \sigma_{+}$ \\
Alors $\exists M,\delta > 0$ et $\mu_0 >0$ tels que $\forall \mu \in ]0,\mu_0], T \geq 1, u\in ]0,1]$ et $(\bftheta, \sigma) \in C(\beta, L, \rho, \sigma_-, \sigma_+)$ 
$$
\norm{\hat{\bftheta}_T(u ; \mu) - \bftheta(u)}_{p,\bftheta,\sigma} \leq M \left( \norm{\bftheta(0)}(1-\delta \mu)^{uT} + \sqrt{\mu} + (T\mu)^{-\beta} \right)
$$
\end{Thm}

\subsection{Analyse en densité spectrale de puissance}
La formule de récurrence \eqref{eq:NLSM} nous fournit à chaque instant $t\in \iseg{0,T}$ un estimateur des coefficient TVAR du processus. Comme dans le cas du processus AR, on peut estimer densité spectrale de puissance mais ici à chaque instants on a une DSP différente.
\begin{Def}[DSP]
La DSP théorique est définie pour $f \in [0,1]$ et $u\in [0,1]$ 
$$
S_x(f ; u) = \frac{1}{\abs{A\left( e^{-2i\pi f} ; u \right)}^2}
$$
où $A(z;u) = 1 - \sum_{k=1}^p a_k(u) z^k$ \\
La DSP estimée est : 
$$
\hat{S}_x(f ; u) = \frac{1}{\abs{\hat{A}\left( e^{-2i\pi f} ; u \right)}^2}
$$
où $\hat{A}(z;u) = 1 - \sum_{k=1}^p \hat{a}_k(u) z^k$
\end{Def}
\begin{Prop}
$\forall u \in [0,1]$, considérons $z_1(u),\cdots, z_p(u)$ les racines de $A(z;u)$ alors en notant $\forall n\in \iseg{1,p}, z_n(u) = \rho_n(u) e^{2i\pi \phi_n(u)}$ avec $\rho_n(u) >0$ et $\phi_n(u) \in \seg{-\frac{1}{2},\frac{1}{2}}$ on a 
$$
S_x(f,u) \text{ admet un pic en f } \iff \exists n\in \iseg{1,p} \, f = \phi_n(u)
$$
\end{Prop}
\begin{proof}
Soit $u\in [0,1]$ alors écrivons $A(z;u) = \prod_{n=1}^p (1 - \frac{z}{z_n(u)}) = $ alors
$$
\abs{A\left( e^{-2i\pi f} ; u \right)} = \prod_{n=1}^p \abs{1 - \rho_n^{-1}(u) e^{-2i\pi (f+\phi_n)}}
$$
Or en utilisant la deuxième inégalité triangulaire : $\forall n \in \iseg{1,p}$ $\abs{1 - \rho_n^{-1}(u) e^{-2i\pi (f+\phi_n)}} \geq \abs{1 - \rho_n^{-1}(u)}$ avec égalité ssi $f = -\phi_n \mod 1$ \\
Donc finalement $\abs{A\left( e^{-2i\pi f} ; u \right)}$ admet ses minima locaux aux $-\phi_n \mod 1$
\end{proof}
\end{document} 
