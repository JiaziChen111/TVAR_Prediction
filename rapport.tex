\documentclass[a4paper,french]{article}

\usepackage{defs}
\usepackage{init}
\title{Travail pour le 20/05}
\date{}
\begin{document}
\maketitle
\section{Révisions processus AR}
\paragraph{Equation AR(p) :} 
\begin{equation}\label{eq:AR}
X_t = \sum_{i=1}^p a_i X_{t-i} + \epsilon_t
\end{equation}
où $\epsilon$ est un bruit blanc centré de variance unitaire.
\subsection{Construction d'une solution stationnaire au second ordre}
\begin{Thm}
Soit $P(z)=1-\sum_{k=1}^p a_k z^k$, supposons que $\forall |z|=1, P(z)\neq 0$ \\
Alors il existe une solution stationnaire au second ordre de \eqref{eq:AR} 
\end{Thm}
\begin{proof}
Pour prouver cela on a besoin du lemme suivant :
\begin{Lem}
Pour $\alpha \in \ell^1$ et $X$ un processus tel que $\sup_t \EE[|X_t|] < +\infty$ on appelle filtrage de $X$ le processus : 
\[ F_\alpha(X) = \left( \sum_{k\in \zset} \alpha_k X_{t-k} \right)_{t\in \zset} \]
Alors si $\alpha,\beta \in \ell^1$ on a $F_\alpha (F_\beta (X)) = X$ si $\alpha \star \beta = \delta$ \\
De plus $\alpha \star \beta = \delta \iff \forall |z|=1 \, \left( \sum_{z\in \zset} \alpha_k z^k \right) \left( \sum_{z\in \zset} \beta_k z^k \right) =1 $ \\
Enfin si $X$ est stationnaire au second ordre $F(X)$ l'est aussi.
\end{Lem}
\begin{Rque}
$F_\alpha = \sum_{k\in \zset} \alpha_i B^k$ où $B$ est l'opérateur de shift $ B : (x_t)_{t\in \zset} \mapsto (x_{t-1})_{t\in \zset}$
\end{Rque}

Le polynôme $P$ peut s'écrire : $P(z)=\prod_{k=1}^p (1-u_k z)$ où les $u_k$ sont les inverses des racines de $P$. L'équation \eqref{eq:AR} s'écrit $P(B)(X)=\epsilon$. Or avec la factorisation obtenue on a : 
\[
P(B) = (1-u_1 B) \circ \cdots \circ (1-u_p B) = F_{\alpha^{(1)}} \circ \cdots \circ F_{\alpha^{(p)}}
\]
où $\alpha_k^{(l)} = \piecewise{
1 & k=0 \\
-u_l & k=1 \\
0 & \text{sinon} \\}
$ \\
Le but désormais est de chercher pour tout $l \in \llbracket 1, p \rrbracket$ un $\beta^{(l)}$ tel que $\alpha^{(l)} \star \beta^{(l)} = \delta$ pour pouvoir inverser la relation. On sait d'après la remarque qu'il suffit de trouver $\beta^{(l)}$ tel que $\frac{1}{1-u_l z} = \sum_{k \in \zset} \beta_k z^k$ pour tout $|z|=1$ \\
On sait de plus que $\forall l \in \llbracket 1, p \rrbracket$ $|u_l| \neq 1$ par hypothèse sur les racines de $P$.\\
Prenons $l\in \llbracket 1,p \rrbracket$ alors deux cas sont possibles :
\begin{itemize}
\item si $|u_l|< 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} = \sum_{k \geq 0} u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
u_l^k & k \geq 0 \\
0 & \text{sinon} \\
}$
\item si $|u_l|> 1$ on a $\forall |z|=1$ $\frac{1}{1-u_l z} \frac{-u_l^{-1} z^{-1}}{1-u_l^{-1} z^{-1}} = \sum_{k \leq -1} -u_l^k z^k$ il suffit donc de prendre $\beta_k^{(l)} = \piecewise{
-u_l^k & k \leq -1 \\
0 & \text{sinon} \\
}$
\end{itemize}
Finalement prenons $F = F_{\beta^{(p)}} \circ \cdots \circ F_{\beta^{(1)}}$, on prends alors 
\[
X = F(\epsilon) 
\]
\end{proof}

\section{Prédiction}
\begin{Prop}
On se donne $X$ stationnaire au second ordre vérifiant \eqref{eq:AR}. On suppose de plus que $X$ est causal i.e $P(z) \neq 0$ pour tout $|z| \leq 1$
On note $\mathcal{H}_t^X = \overline{\mathrm{Vect}\ens{X_s, s\leq t}}$. Alors
\[ \hat{X}_{t+1} = proj(X_{t+1} | \mathcal{H}_t^X ) = \sum_{k=1}^p a_k X_{t+1-k} \]
\end{Prop}
\begin{proof}
$X_{t+1} = \sum_{k=1}^p a_k X_{t+1-k} +\epsilon_{t+1}$ de plus $\forall k \in \llbracket 1,p \rrbracket \, X_{t+1-k} \in \mathcal{H}_t^X$ donc
\[ \hat{X}_{t+1} = \sum_{k=1}^p a_k X_{t+1-k} + proj(\epsilon_{t+1} | \mathcal{H}_t^X ) \]
Or $X$ est causal donc dans la construction de $X$ (cf preuve d'avant) on a $P(z)= \prod_{l=1}^p (1-u_l z)$ où $\forall l |u_l|<1$ ainsi les $\beta^{(l)}$ correspondants sont tous à support dans $\nset$ ce qui entraine que $\psi = \beta^{(p)} \star \cdots \star \beta^{(1)}$ est aussi à support dans $\nset$ et donc 
\[ X_t = \sum_{k=0}^{+\infty} \psi_k \epsilon_{t-k}\] On en déduit que $\mathcal{H}_t^X = \mathcal{H}_t^\epsilon$ et donc comme $\epsilon$ est un bruit blanc $proj(\epsilon_{t+1} | \mathcal{H}_t^X ) = proj(\epsilon_{t+1} | \mathcal{H}_t^\epsilon ) = 0$ d'où la solution
\end{proof}

\section{TVAR}
\paragraph{Equation TVAR :}
\begin{equation} \label{eq:TVAR}
X_t = \sum_{i=1}^p a_i(t) X_{t-i} + \sigma(t) \epsilon_t
\end{equation}
\paragraph{Condition de stabilité :}
Le critère qui nous intéresse est d'avoir une solution de \eqref{eq:TVAR} vérifiant la condition de stabilité suivante :
\begin{equation} \label{eq:stabilite}
\sup_{t\in \zset} \EE[|X_t|^2] < +\infty
\end{equation}
\subsection{Cas simple}
On considère ici $\sigma(t)=1 \, \forall t\in \rset$ et $a_i(t) = \piecewise{
0 & t<0 \\
a_i & t \geq 0 \\ 
} $. 
\begin{Prop}\label{prop:cas_simple_unicite}
Dans ce cas particulier, l'équation \eqref{eq:TVAR} admet une unique solution
\end{Prop} 
\begin{proof}
Tout d'abord pour $t < 0$ on a $X_t = \epsilon_t$ \\
Pour $t \geq 0$, notons $\mb{X}_k = [ X_k, \cdots , X_{k-p+1} ]^T$, $\mb{e}_1 = [1,0, \cdots, 0]^T$ et $A = \begin{pmatrix}
a_1 & a_2 & \cdots & \cdots & a_p \\
1 & 0 & \cdots & \cdots & 0 \\
0 & \ddots & \ddots & & \vdots \\
\vdots & \ddots & \ddots & \ddots & 0 \\
0 & \cdots & 0 & 1 & 0
\end{pmatrix}$. Alors l'équation \eqref{eq:TVAR} s'écrit : 
\[ \mb{X}_t = A\mb{X}_{t-1} + \epsilon_t \mb{e}_1 \]
En itérant pour $t-1, t-2$ etc, on obtient 
\[ \forall k\geq 0 \,  \mb{X}_t = A^{k+1} \mb{X}_{t-k-1} + \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  \]
Considérons $k \geq t$ alors $\mb{X}_{t-k-1} = [ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]^T$. Ainsi 
\begin{equation}\label{eq:X}
\forall k\geq t \,  \mb{X}_t = A^{k+1} \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix} 
+ \sum_{j=0}^k \epsilon_{t-j} A^j \mb{e}_1  
\end{equation}
ce qui fournit une définition $X_t = \mb{e}_1^T \mb{X}_t$ unique
\end{proof}

On cherche alors une condition sur les $(a_i)_{i=1}^p$ pour cette solution vérifie la condition de stabilité \eqref{eq:stabilite}


\subsubsection{Cas où p=1}
\begin{Prop}
Si $p=1$ on note $a(t)= \piecewise{
0 & t<0 \\
a & t \geq 0 \\} $ et \eqref{eq:TVAR} devient $X_t = a(t)X_{t-1} + \sigma(t) \epsilon_t$. La solution de l'équation vérifie la condition de stabilité \eqref{eq:stabilite} si et seulement si $|a| < 1$
\end{Prop}
\begin{proof}
Si $t < 0\, X_t = \epsilon_t$ donc $\sup_{t < 0} \EE[|X_t|^2] = 1$. Si $t\geq 0$, la formule de $X_t$ donnée par \eqref{eq:X} se traduit pour $p=1$ par
$ \forall k \geq t \,  X_t = a^{k+1}\epsilon_{t-k-1} + \sum_{j=0}^k a^j \epsilon_{t-j} $ i.e $\forall k \geq t \, X_t = \sum_{j=0}^{k+1} a^j\epsilon_{t-j}$.
Ainsi 
\[ \forall t \in \nset, \, \forall k \geq t \, \EE[|X_t|^2] = \sum_{i=0}^{k+1} \sum_{j=0}^{k+1} a^i \overline{a}^j \EE[\epsilon_{t-i} \epsilon_{t-j}]= \sum_{i=0}^{k+1} |a|^{2i} \]
Ainsi en faisant tendre $k$ vers $+\infty$ on obtient 
 \[ \forall t \in \nset, \, \EE[|X_t|^2] = \sum_{i=0}^{+\infty} |a|^{2i} = \piecewise{
+\infty & |a|\geq 1 \\
\frac{1}{1-|a|^2} & |a| < 1} \]
Ce qui donne $\sup_{t\in \nset} \EE[|X_t|^2] < +\infty \iff |a| < 1$ et comme sur $\sup_{t < 0} \EE[|X_t|^2] = 1$ la condition est valable pour le sup sur $t\in \zset$
\end{proof}

\subsubsection{Cas p quelconque}
Pour prouver ce cas, quelques lemmes d'algèbre linéaire sont nécessaires
\begin{Lem}
La matrice $A$ définie dans la preuve de la propriété \ref{prop:cas_simple_unicite} a pour polynôme caractéristique : 
\[
\chi_A (X)= \det(A-XI_p) = (-1)^p \left( X^p - \sum_{i=1}^p a_i X^{p-i} \right)
\]
\end{Lem}
\paragraph{Conséquence :}
Les valeurs propres de $A$ sont les inverses des racines de $P = 1- \sum_{i=1}^p a_i X^i$
car $\chi_A = (-1)^p X^p P\left( \frac{1}{X}\right)$
\begin{Lem}\label{lem:approx_rayon_spec}
Soit $A \in \cset^{p\times p}$ alors, on rappelle la définition de la norme subordonnée associée à une norme $\norm{.}$ sur $\cset^p$ : \[
\tnorm{A} = \sup_{\norm{x} = 1} \norm{Ax}
\]
On rappelle aussi la définition du rayon spectral $ \rho(A) = \max_{\lambda \in spec(A)} \abs{\lambda}$. On a la propriété suivante : \\
Pour tout $A \in \cset^{p\times p}$, pour tout  $\epsilon >0$ il existe une norme sur $\cset^p$ dépendant de $\epsilon$ et de $A$ telle que la norme subordonnée correspondante $\tnorm{.}_{\epsilon,A}$ vérifie
\[
\tnorm{A}_{\epsilon,A} \leq \rho(A) + \epsilon
\]
\end{Lem}
\begin{Prop}[Condition suffisante]
Dans ce cas particulier, en notant $P(z) = 1 - \sum_{i=1}^p a_i z^i$, si $\forall |z| \leq 1\, P(z)\neq 0$ (i.e les racines de $P$ sont hors du disque unité fermé) alors la solution de l'équation \eqref{eq:TVAR} vérifie la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
On part de la définition de $\mb{X}_t$ pour $t \in \nset $ donnée par \eqref{eq:X}

De plus $X_t = \mb{e}_1^T \mb{X}_t$ donc $\EE[|X_t|^2] = \EE[X_t \overline{X}_t^T] = \mb{e}_1^T\EE[\mb{X}_t \mb{X}_t^T] \mb{e}_1$ avec $\forall k \geq t$, comme $\epsilon$ est un bruit blanc
\begin{align*}
\EE[\mb{X}_t \mb{X}_t^T]  
&= \sum_{j=0}^k \sum_{l=0}^k A^j \mb{e}_1 \EE[\epsilon_{t-j} \epsilon_{t-l}] \mb{e}_1^T {(A^l)}^T  + A^{k+1} \EE[ \begin{pmatrix}
\epsilon_{t-k-1} \\
\vdots \\
\epsilon_{t-k-p}\\
\end{pmatrix}  {[ \epsilon_{t-k-1}, \cdots , \epsilon_{t-k-p} ]} ] {(A^{k+1})}^T \\
& = \sum_{j=0}^k A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T + A^{k+1} {(A^{k+1})}^T
\end{align*}
Or on a supposé que les racines de $P$ sont de module strictement supérieur à 1 donc les valeurs propres de $A$ sont de module strictement inférieur à 1. Ainsi $\rho(A) < 1$, il existe donc $\epsilon > 0$ tel que $\rho(A) + \epsilon < 1$. En appliquant le lemme \ref{lem:approx_rayon_spec} à $\epsilon$ et $A$ on obtient (en notant juste $\tnorm{A}$ au lieu de $\tnorm{A}_{A,\epsilon}$ pour alléger les notations) $\tnorm{A} < 1$. \\
Ceci implique dans un premier temps $\tnorm{A^k} \leq \tnorm{A}^k \xrightarrow[k \rightarrow +\infty]{} 0$ donc $A^k \xrightarrow[k \rightarrow +\infty]{} 0$ et ainsi $A^{k+1} (A^{k+1})^T \xrightarrow[k \rightarrow +\infty]{} 0$ \\
De plus $\forall j\in \nset$ $\tnorm{A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T}
\leq \tnorm{A}^{2j} \tnorm{\mb{e}_1 \mb{e}_1^T}$  qui est terme général d'une série convergence dans $\rset$ car $\tnorm{A} < 1$ donc la série des $A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T$ est absolument convergente donc convergente dans $\rset^{p\times p}$. \\
En faisant donc tendre $k$ vers $+\infty$ dans l'expression de $\EE[\mb{X}_t \mb{X_t}^T]$ on obtient : 
\[
\EE[\mb{X}_t \mb{X_t}^T] = \sum_{j=0}^{+\infty} A^j \mb{e}_1\mb{e}_1^T{(A^j)}^T \in \rset^{p\times p}
\]
Ainsi $\EE[|X_t|^2]$ étant le premier coefficient de cette matrice, $\EE[|X_t|^2] < +\infty$
\end{proof}
\subsection{Cas général avec p=1}
L'équation \eqref{eq:TVAR} devient $X_t = a(t)X_{t-1} + \sigma(t) \epsilon_t$
\begin{Prop}
Si $\sup_t |a(t)| < 1$ et $\sup_t |\sigma(t)| < +\infty$ alors il existe un unique processus $(X_t)_{t \in \zset}$ vérifiant à la fois \eqref{eq:TVAR} et la condition de stabilité \eqref{eq:stabilite}
\end{Prop}
\begin{proof}
En itérant $k$ fois l'équation on obtient 
\begin{equation}\label{eq:p_1_iter} 
\forall k \geq 0 \, X_t = \left( \prod_{j=0}^{k} a(t-j)  \right) X_{t-k-1} + \sum_{j=0}^k \left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)\epsilon_{t-j}
\end{equation}
\begin{itemize}
\item Supposons que $X$ vérifie la condition de stabilité et appelons $M = \sup_t \EE[\abs{X_t}^2]$, $a_{\max} = \sup_t |a(t)|$ et $\sigma_{\max} = \sup_t |\sigma(t)|$ alors on a $\forall k \geq 0$ :
\[
\norm{\left( \prod_{j=0}^{k} a(t-j)  \right) X_{t-k-1} }_2^2 = \left( \prod_{j=0}^{k} \abs{a(t-j)}^2 \right) \EE[\abs{X_{t-k-1}}^2] \leq a_{\max}^{2(k+1)} M
\]
Ainsi $\lim_{k \rightarrow +\infty} \norm{\left( \prod_{j=0}^{k} a(t-j)  \right) X_{t-k-1} }^2 = 0$ car on a pris $a_{\max} < 1$.\\ 
De plus $\forall j \geq 0$
\begin{equation}\label{eq:p_1_sommabilite}
\norm{\left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2^2 =  \left( \prod_{i=0}^{j-1} \abs{a(t-i)}^2 \right) \abs{\sigma(t-j)}^2 \EE[\abs{\epsilon_{t-j}}^2]
= \left( \prod_{i=0}^{j-1} \abs{a(t-i)}^2 \right) \abs{\sigma(t-j)}^2
\leq a_{\max}^{2j} \sigma_{\max}^2
\end{equation}
Ce qui donne 
\[
\norm{\left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)\epsilon_{t-j}}_2 \leq a_{\max}^{j} \sigma_{\max}
\]
Encore une fois, parce qu'on a pris $a_{\max}<1$, on a à droite de l'inégalité le terme général d'une série convergente. Ainsi en notant $c_j = \left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)$ la série de terme général $c_j \epsilon_{t-j}$ est absolument convergente donc convergente. \\
On peut alors faire tendre $k$ vers $+\infty$ dans \eqref{eq:p_1_iter}, on obtient 
\begin{equation}\label{eq:p_1_solution}
X_t = \sum_{j=0}^{+\infty} c_j  \epsilon_{t-j} \text{ avec } c_j = \left( \prod_{i=0}^{j-1} a(t-i) \right) \sigma(t-j)
\end{equation}
\item Définissons maintenant $X$ comme dans l'équation \eqref{eq:p_1_solution} et montrons qu'elle vérifie l'équation TVAR et la condition de stabilité. \\
Tout d'abord pour tout $t\in \zset$ on a 
\begin{align*}
a(t) X_{t-1} + \sigma(t) \epsilon_t 
&= a(t) \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} a(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} +\sigma(t) \epsilon_t \\
&= \sum_{j=-1}^{+\infty} \left( \prod_{i=-1}^{j-1} a(t-1-i) \right)\sigma(t-1-j)\epsilon_{t-1-j} \\
&\underset{(j \leftarrow j+1)}{=}  \sum_{j=0}^{+\infty} \left( \prod_{i=-1}^{j-2} a(t-1-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&\underset{(i \leftarrow i+1)}{=} \sum_{j=0}^{+\infty} \left( \prod_{i=0}^{j-1} a(t-i) \right)\sigma(t-j)\epsilon_{t-j} \\
&= X_t
\end{align*}
\item Remarquons tout d'abord que \eqref{eq:p_1_sommabilite} prouve que $(c_j)_{j\in \nset} \in \ell^2(\nset)$. On a alors $\forall t\in \zset$ 
\begin{align*}
\EE[\abs{X_t}^2] 
&= \EE[\abs{\lim_{k \rightarrow +\infty} \sum_{j=0}^k c_j \epsilon_{t-j}}^2] \\
&= \lim_{k \rightarrow +\infty} \EE[\abs{\sum_{j=0}^k c_j \epsilon_{t-j}}^2] \text{ (par continuité de l'espérance)}\\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \EE[\abs{\epsilon_{t-j}}^2] \text{ (car } \epsilon \text{ est un bruit blanc) } \\
&= \lim_{k \rightarrow +\infty} \sum_{j=0}^k \abs{c_j}^2 \\
&= \sum_{k=0}^{+\infty}  \abs{c_j}^2
\end{align*}
Ce résultat étant indépendant de $t$ on a bien 
\[
\sup_t \EE[\abs{X_t}^2] = \sum_{k=0}^{+\infty}  \abs{c_j}^2 < +\infty
\]
\end{itemize}

\end{proof}
\subsection{Contre-exemple avec p=2}

On va montrer que la condition obtenue pour le cas $p=1$ n'est plus suffisante pour $p=2$.

Pour ce contre-exemple, on va considérer le TVAR(2) définit comme ceci :
\begin{align*}
X_{2t} = a X_{2t-1} + \epsilon_{2t} \\
X_{2t+1} = b_1 X{2t} + b_2 X_{2t-1} + \epsilon_{2t+1}
\end{align*}

pour $t>0$ sinon $X_t = \epsilon_t$.

On a alors :
\begin{equation}
\forall t>0, X_{2t+1} = (ab_1 + b_2) X_{2t-1} + b_1 \epsilon_{2t} + \epsilon_{2t+1}
\end{equation}
On a alors que le processus $Y_t = X_{2t+1}$ suit l'équation d'un AR(1) dont la condition de stabilité implique :
\begin{equation}
|ab_1 + b_2| < 1 (*)
\end{equation}

D'autre part, le polynôme caractéristique associé est $P(z) = 1 - b_1 z - b_2 z^2$. Posons $P(z) = (1-bz)^2$. On a alors $b_1 = 2b$ et $b_2 = -b^2$.
La condition $(*)$ donne:
\begin{equation}
|2ba-b^2| <1 (*)
\end{equation}
Avec $a = -b$, celle-ci devient :
\begin{equation}
3b^2 < 1
\end{equation}
Or ceci peut être faux. Il suffit de prendre $b=\frac{1}{\sqrt(2)}$ par exemple.

On a donc trouvé une sous-suite $Y_t$ du processus $X_t$ qui diverge. Donc la condition initiale, à savoir $sup_t|a_i(t)|<1$, n'est plus suffisante pour assurer la stabilité d'un TVAR(2).


\end{document} 
